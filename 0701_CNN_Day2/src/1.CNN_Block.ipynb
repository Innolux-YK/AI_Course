{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.CNN_Block.ipynb 說明\n",
    "本 notebook 主要教學如何用 PyTorch 建立卷積神經網路（CNN）的基本區塊，並進行簡單測試。內容重點如下：\n",
    "1. **安裝與匯入套件**：安裝 torchinfo（用於顯示模型結構），匯入 PyTorch 相關模組與 summary 工具。\n",
    "2. **裝置選擇**：自動偵測是否有 GPU（cuda），有則用 GPU，否則用 CPU。\n",
    "3. **啟動函數工具**：定義 activation_func，可根據字串選擇不同 activation function（如 relu、leaky_relu、sigmoid、prelu、softmax、gelu）。\n",
    "4. **CNNBlock 類別**：自訂 CNNBlock 類別，包含卷積層、BatchNorm 層、啟動函數，並定義 forward 方法（卷積→啟動→正規化）。\n",
    "5. **CNNBlock 測試**：建立隨機輸入張量，實例化 CNNBlock，將輸入丟進模型並印出輸出 shape。\n",
    "6. **更完整的 CNN 模型結構**：定義 ModelStructure 類別，堆疊多個 CNNBlock 與池化層，最後接全連接層（FC），可用於分類任務。\n",
    "7. **模型結構摘要**：用 summary 工具顯示整個模型的結構、每層的輸入/輸出 shape 及參數數量。\n",
    "本 notebook 讓你熟悉如何用 PyTorch 自訂 CNN 卷積區塊、組合成完整模型，並進行 forward 測試與結構檢查，是 CNN 架構設計的基礎教學範例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7092,
     "status": "ok",
     "timestamp": 1696475002417,
     "user": {
      "displayName": "Guffrey Gu",
      "userId": "09202003599580992976"
     },
     "user_tz": -480
    },
    "id": "C10oDW1CyeN6",
    "outputId": "101a35d4-1c47-44a5-a8cc-27e70ed834a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# 安裝 torchinfo 套件，用於顯示 PyTorch 模型的結構與參數資訊\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yIMRi80Rx_LV"
   },
   "outputs": [],
   "source": [
    "# 匯入必要的套件\n",
    "import torch.nn as nn  # 匯入 PyTorch 的神經網路模組，方便建立神經網路結構\n",
    "import torch           # 匯入 PyTorch 主套件，進行張量運算與 GPU 加速\n",
    "from torchinfo import summary  # 匯入 summary 函數，用於顯示模型結構摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o-6Lyl7Dx_La"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選擇運算裝置 (GPU 或 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 若有 GPU 則使用 GPU，否則使用 CPU\n",
    "device  # 顯示目前使用的裝置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gxUcSNqFx_La"
   },
   "outputs": [],
   "source": [
    "# 匯入必要的套件\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定義一個函數，根據傳入的 activation 字串回傳對應的啟動函數 (Activation Function) 物件\n",
    "def activation_func(activation):\n",
    "    return nn.ModuleDict({\n",
    "        'relu': nn.ReLU(inplace=True),   # ReLU 激活函數，inplace=True 代表直接在原 tensor 上修改，節省記憶體\n",
    "        'leaky_relu': nn.LeakyReLU(negative_slope=0.01, inplace=True),  # LeakyReLU，負值部分有微小斜率\n",
    "        'sigmoid': nn.Sigmoid(),         # Sigmoid 激活函數，常用於二元分類輸出\n",
    "        'prelu': nn.PReLU(),             # PReLU，負值部分的斜率可學習\n",
    "        'softmax': nn.Softmax(dim=1),    # Softmax，常用於多分類輸出，dim=1 代表對每個樣本的 channel 做 softmax\n",
    "        'gelu': nn.GELU()                # GELU，常用於較新型的神經網路架構\n",
    "    })[activation]  # 根據傳入的 activation 字串回傳對應的物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ov96akKUx_Lb"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定義一個 CNNBlock 類別，繼承自 nn.Module，作為卷積神經網路的基本區塊\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, k_size, activation='relu', pad=1, s=1, dilation=1):\n",
    "        super().__init__()\n",
    "        # 定義 2D 卷積層\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel,\n",
    "                              k_size, padding=pad, stride=s, dilation=dilation) # Group_channel\n",
    "        # 定義 Batch Normalization 層，幫助加速訓練與穩定收斂\n",
    "        self.batchNorm = nn.BatchNorm2d(out_channel)\n",
    "        # 定義啟動函數 (Activation Function)\n",
    "        self.actfunction = activation_func(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向傳播流程：先卷積 -> 啟動函數 -> 批次正規化\n",
    "        x = self.conv(x)           # 卷積運算\n",
    "        x = self.actfunction(x)    # 啟動函數\n",
    "        x = self.batchNorm(x)      # 批次正規化\n",
    "        return x\n",
    "# bn 跟 activation function能不能換位置\n",
    "# forward這個名字能不能換掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TB2yhWEix_Lc",
    "outputId": "db66df33-b92b-4ef3-b337-3dad833b7b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 測試 CNNBlock 卷積區塊\n",
    "input = torch.randn(1, 64, 3, 3) # 建立一個隨機輸入張量，形狀為 (batch=1, channel=64, 高=3, 寬=3)\n",
    "model = CNNBlock(64, 128, 3)     # 建立一個 CNNBlock，輸入通道 64，輸出通道 128，卷積核大小 3x3\n",
    "# summary(model, input_data=input) # 可以用 summary 查看模型結構 (此行目前註解掉)\n",
    "output = model(input)             # 將輸入資料傳入模型，取得輸出\n",
    "print(output.size())              # 印出輸出張量的形狀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wJA7SXczx_Lk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 定義一個更完整的 CNN 模型結構\n",
    "class ModelStructure(nn.Module):\n",
    "    def __init__(self, cls):\n",
    "        super().__init__()\n",
    "        # 第一層卷積區塊，輸入 3 通道，輸出 6 通道，卷積核 5x5，無 padding\n",
    "        self.conv1 = CNNBlock(in_channel = 3, out_channel = 6, k_size = 5, activation='relu', pad = 0)\n",
    "        # 第二層卷積區塊，輸入 6 通道，輸出 16 通道，卷積核 5x5，無 padding\n",
    "        self.conv2 = CNNBlock(in_channel = 6, out_channel = 16, k_size = 5, activation='relu', pad = 0)\n",
    "        # 第三層卷積區塊，輸入 16 通道，輸出 120 通道，卷積核 3x3，無 padding\n",
    "        self.conv3 = CNNBlock(in_channel = 16, out_channel = 120, k_size = 3, activation='relu', pad = 0)\n",
    "\n",
    "        # 定義最大池化層，每次將特徵圖縮小一半\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # 定義 ReLU 啟動函數\n",
    "        self.act = nn.ReLU(True)\n",
    "\n",
    "        # 定義全連接層 (Fully Connected Layer)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features = 120*25*25, out_features = 84),  # 第一層全連接，輸入 120*25*25，輸出 84\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features = 84, out_features = cls),         # 第二層全連接，輸出為分類數量\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向傳播流程：卷積區塊 + 啟動函數 + 池化，重複三次\n",
    "        x = self.pool(self.act(self.conv1(x)))\n",
    "        x = self.pool(self.act(self.conv2(x)))\n",
    "        x = self.pool(self.act(self.conv3(x)))\n",
    "        # print(x.size())  # 可用於檢查特徵圖尺寸\n",
    "        x = x.view(-1, 120*25*25) # 將多維特徵圖展平成一維向量 (也可用 .flatten())\n",
    "        x = self.fc(x)            # 通過全連接層\n",
    "        return x\n",
    "\n",
    "# 建立模型，分類數量設為 2，並移至指定裝置 (GPU/CPU)\n",
    "cnn_model = ModelStructure(cls=2).to(device)\n",
    "\n",
    "# 建立測試用的隨機張量，形狀為 (batch=1, channel=3, 高=224, 寬=224)，並移至裝置\n",
    "test_tensor = torch.randn((1, 3, 224, 224)).to(device)\n",
    "output = cnn_model(test_tensor)  # 將測試資料傳入模型，取得輸出\n",
    "print(output.size())             # 印出輸出張量的形狀\n",
    "\n",
    "# 25*25 ?  # 註解：經過三次池化後，原本 224x224 會縮小為 25x25\n",
    "# nn.ReLU(True)?  # 註解：inplace=True 代表直接在原 tensor 上修改\n",
    "# why .view()不會多存一份copy()? reshape()會存一份copy?  # 註解：.view() 只改變形狀，不複製資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mWuWJ7bGx_Ll"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ModelStructure                           [1, 2]                    --\n",
       "├─CNNBlock: 1-1                          [1, 6, 220, 220]          --\n",
       "│    └─Conv2d: 2-1                       [1, 6, 220, 220]          456\n",
       "│    └─ReLU: 2-2                         [1, 6, 220, 220]          --\n",
       "│    └─BatchNorm2d: 2-3                  [1, 6, 220, 220]          12\n",
       "├─ReLU: 1-2                              [1, 6, 220, 220]          --\n",
       "├─MaxPool2d: 1-3                         [1, 6, 110, 110]          --\n",
       "├─CNNBlock: 1-4                          [1, 16, 106, 106]         --\n",
       "│    └─Conv2d: 2-4                       [1, 16, 106, 106]         2,416\n",
       "│    └─ReLU: 2-5                         [1, 16, 106, 106]         --\n",
       "│    └─BatchNorm2d: 2-6                  [1, 16, 106, 106]         32\n",
       "├─ReLU: 1-5                              [1, 16, 106, 106]         --\n",
       "├─MaxPool2d: 1-6                         [1, 16, 53, 53]           --\n",
       "├─CNNBlock: 1-7                          [1, 120, 51, 51]          --\n",
       "│    └─Conv2d: 2-7                       [1, 120, 51, 51]          17,400\n",
       "│    └─ReLU: 2-8                         [1, 120, 51, 51]          --\n",
       "│    └─BatchNorm2d: 2-9                  [1, 120, 51, 51]          240\n",
       "├─ReLU: 1-8                              [1, 120, 51, 51]          --\n",
       "├─MaxPool2d: 1-9                         [1, 120, 25, 25]          --\n",
       "├─Sequential: 1-10                       [1, 2]                    --\n",
       "│    └─Linear: 2-10                      [1, 84]                   6,300,084\n",
       "│    └─ReLU: 2-11                        [1, 84]                   --\n",
       "│    └─Linear: 2-12                      [1, 2]                    170\n",
       "==========================================================================================\n",
       "Total params: 6,320,810\n",
       "Trainable params: 6,320,810\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 100.77\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 12.52\n",
       "Params size (MB): 25.28\n",
       "Estimated Total Size (MB): 38.40\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示模型結構摘要，包含每層的輸入/輸出形狀與參數數量\n",
    "summary(cnn_model, input_data=test_tensor)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
