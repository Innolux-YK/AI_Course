{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74ea49",
   "metadata": {},
   "source": [
    "# 中風預測模型：訓練與評估\n",
    "\n",
    "本筆記本旨在建立一個深度學習模型，用於根據患者的健康數據預測其中風風險。流程包括：\n",
    "1.  **資料載入與探索**：載入訓練與測試資料集，並進行初步的探索性分析。\n",
    "2.  **資料前處理與特徵工程**：清理資料、處理缺失值，並建立有助於模型學習的新特徵。\n",
    "3.  **模型建立**：使用 PyTorch 建立一個深度神經網路 (DNN)。\n",
    "4.  **模型訓練**：設定損失函數、優化器，並在訓練資料上進行模型訓練與驗證。\n",
    "5.  **結果評估**：使用驗證集評估模型效能，並視覺化訓練過程。\n",
    "6.  **產生預測**：使用訓練好的最佳模型對測試集進行預測，並產生提交檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def786a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (2.3.0)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\yukai01.lin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "目前使用的裝置為: cpu\n"
     ]
    }
   ],
   "source": [
    "# 安裝與匯入所需套件\n",
    "\n",
    "# 安裝 imblearn 套件，用於處理不平衡資料\n",
    "!pip install -U imblearn\n",
    "\n",
    "# --- 系統與資料處理 ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 機器學習與資料前處理 ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# --- 深度學習 (PyTorch) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "# --- 視覺化 ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- 設定裝置與隨機種子 ---\n",
    "# 檢查是否有可用的 CUDA GPU，若有則使用 GPU，否則使用 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"目前使用的裝置為: {device}\")\n",
    "\n",
    "# 設定隨機種子以確保實驗可重複性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624f38b",
   "metadata": {},
   "source": [
    "## 1. 資料載入與探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6362e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 訓練資料集資訊 ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4088 entries, 0 to 4087\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 4088 non-null   int64  \n",
      " 1   gender             4088 non-null   object \n",
      " 2   age                4088 non-null   float64\n",
      " 3   hypertension       4088 non-null   int64  \n",
      " 4   heart_disease      4088 non-null   int64  \n",
      " 5   ever_married       4088 non-null   object \n",
      " 6   work_type          4088 non-null   object \n",
      " 7   Residence_type     4088 non-null   object \n",
      " 8   avg_glucose_level  4088 non-null   float64\n",
      " 9   bmi                3928 non-null   float64\n",
      " 10  smoking_status     4088 non-null   object \n",
      " 11  stroke             4088 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 383.4+ KB\n",
      "\n",
      "==============================\n",
      "\n",
      "--- 測試資料集資訊 ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1022 entries, 0 to 1021\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1022 non-null   int64  \n",
      " 1   gender             1022 non-null   object \n",
      " 2   age                1022 non-null   float64\n",
      " 3   hypertension       1022 non-null   int64  \n",
      " 4   heart_disease      1022 non-null   int64  \n",
      " 5   ever_married       1022 non-null   object \n",
      " 6   work_type          1022 non-null   object \n",
      " 7   Residence_type     1022 non-null   object \n",
      " 8   avg_glucose_level  1022 non-null   float64\n",
      " 9   bmi                981 non-null    float64\n",
      " 10  smoking_status     1022 non-null   object \n",
      "dtypes: float64(3), int64(3), object(5)\n",
      "memory usage: 88.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 從 CSV 檔案讀取訓練與測試資料集\n",
    "data_train = pd.read_csv('./data/train.csv', index_col=False)\n",
    "data_test = pd.read_csv(\"./data/test.csv\", index_col=False)\n",
    "\n",
    "# 顯示訓練資料集的資訊，檢查欄位型態與缺失值\n",
    "print(\"--- 訓練資料集資訊 ---\")\n",
    "data_train.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 顯示測試資料集的資訊\n",
    "print(\"--- 測試資料集資訊 ---\")\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52aefc5",
   "metadata": {},
   "source": [
    "## 2. 資料前處理與特徵工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae22b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料前處理完成！\n",
      "特徵數量: 37\n"
     ]
    }
   ],
   "source": [
    "# 定義優化的資料預處理函式\n",
    "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    對輸入的 DataFrame 進行預處理與特徵工程。\n",
    "    \n",
    "    Args:\n",
    "        data: 輸入的原始資料 DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        處理後的 DataFrame\n",
    "        \n",
    "    處理步驟：\n",
    "    1. 處理 'bmi' 欄位的缺失值 (使用中位數填充)。\n",
    "    2. 根據 'age' 建立年齡分組特徵 'age_group'。\n",
    "    3. 建立高風險年齡標記 'high_risk_age' (>=65歲)。\n",
    "    4. 根據 'bmi' 建立 BMI 分組特徵 'bmi_group'。\n",
    "    5. 根據 'avg_glucose_level' 建立血糖分組特徵 'glucose_group'。\n",
    "    6. 建立多重風險因子 'multiple_risks' (高血壓 + 心臟病 + 高風險年齡)。\n",
    "    7. 建立慢性病組合 'chronic_diseases' (高血壓 + 心臟病)。\n",
    "    8. 添加交互作用特徵。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = data.copy()\n",
    "        \n",
    "        # 1. 處理缺失值 - BMI (使用中位數填充)\n",
    "        if df['bmi'].isnull().sum() > 0:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df['bmi'] = imputer.fit_transform(df[['bmi']]).flatten()\n",
    "            print(f\"填充了 {data['bmi'].isnull().sum()} 個 BMI 缺失值\")\n",
    "        \n",
    "        # 2. 年齡分組 (更細緻的分組)\n",
    "        df['age_group'] = pd.cut(df['age'], \n",
    "                                bins=[0, 18, 30, 45, 60, 75, 100], \n",
    "                                labels=['<18', '18-30', '31-45', '46-60', '61-75', '>75'])\n",
    "        \n",
    "        # 3. 高風險年齡標記\n",
    "        df['high_risk_age'] = (df['age'] >= 65).astype(int)\n",
    "        \n",
    "        # 4. BMI 分組 (更細緻的分組)\n",
    "        df['bmi_group'] = pd.cut(df['bmi'], \n",
    "                                bins=[0, 18.5, 25, 30, 35, 100], \n",
    "                                labels=['underweight', 'normal', 'overweight', 'obese_1', 'obese_2'])\n",
    "        \n",
    "        # 5. 血糖分組 (更細緻的分組)\n",
    "        df['glucose_group'] = pd.cut(df['avg_glucose_level'], \n",
    "                                    bins=[0, 100, 126, 180, 250, 400], \n",
    "                                    labels=['normal', 'prediabetes', 'diabetes', 'high', 'very_high'])\n",
    "        \n",
    "        # 6. 多重風險因子組合\n",
    "        df['multiple_risks'] = (df['hypertension'] + df['heart_disease'] + df['high_risk_age']).astype(int)\n",
    "        \n",
    "        # 7. 慢性病組合\n",
    "        df['chronic_diseases'] = (df['hypertension'] + df['heart_disease']).astype(int)\n",
    "        \n",
    "        # 8. 新增交互作用特徵\n",
    "        df['age_bmi_interaction'] = df['age'] * df['bmi']\n",
    "        df['glucose_age_interaction'] = df['avg_glucose_level'] * df['age']\n",
    "        df['risk_score'] = (df['hypertension'] * 0.3 + \n",
    "                           df['heart_disease'] * 0.4 + \n",
    "                           df['high_risk_age'] * 0.3)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"資料預處理時發生錯誤: {e}\")\n",
    "        raise\n",
    "\n",
    "# 定義優化的類別特徵編碼函式\n",
    "def encode_categorical_features(train_df: pd.DataFrame, \n",
    "                               test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    對類別特徵進行 One-Hot Encoding。\n",
    "    確保訓練集和測試集有相同的特徵欄位。\n",
    "    \n",
    "    Args:\n",
    "        train_df: 訓練資料 DataFrame\n",
    "        test_df: 測試資料 DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        編碼後的訓練資料、測試資料和特徵欄位列表的元組\n",
    "    \"\"\"\n",
    "    try:\n",
    "        categorical_features = ['gender', 'ever_married', 'work_type', 'Residence_type', \n",
    "                               'smoking_status', 'age_group', 'bmi_group', 'glucose_group']\n",
    "        \n",
    "        # 使用 get_dummies 進行 one-hot encoding\n",
    "        train_encoded = pd.get_dummies(train_df, columns=categorical_features, \n",
    "                                      prefix=categorical_features, drop_first=True)\n",
    "        test_encoded = pd.get_dummies(test_df, columns=categorical_features, \n",
    "                                     prefix=categorical_features, drop_first=True)\n",
    "        \n",
    "        # 確保訓練和測試數據有相同的特徵欄位\n",
    "        train_cols = set(train_encoded.columns)\n",
    "        test_cols = set(test_encoded.columns)\n",
    "        \n",
    "        missing_in_test = list(train_cols - test_cols)\n",
    "        for col in missing_in_test:\n",
    "            if col not in ['stroke']: # 不在測試集中加入目標變數\n",
    "                test_encoded[col] = 0\n",
    "                \n",
    "        missing_in_train = list(test_cols - train_cols)\n",
    "        for col in missing_in_train:\n",
    "            train_encoded[col] = 0\n",
    "\n",
    "        # 確保欄位順序一致\n",
    "        feature_order = sorted([col for col in train_encoded.columns if col not in ['id', 'stroke']])\n",
    "        \n",
    "        print(f\"編碼完成，總共有 {len(feature_order)} 個特徵\")\n",
    "        return train_encoded, test_encoded, feature_order\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"特徵編碼時發生錯誤: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- 執行資料前處理 ---\n",
    "print(\"開始資料前處理...\")\n",
    "\n",
    "# 1. 應用預處理與特徵工程\n",
    "train_processed = preprocess_data(data_train)\n",
    "test_processed = preprocess_data(data_test)\n",
    "\n",
    "# 2. 應用類別變數編碼\n",
    "train_encoded, test_encoded, feature_order = encode_categorical_features(train_processed, test_processed)\n",
    "\n",
    "# 3. 準備訓練特徵 (X) 與目標 (y)\n",
    "X_features = train_encoded[feature_order]\n",
    "y_stroke = train_encoded['stroke']\n",
    "\n",
    "# 4. 準備測試特徵\n",
    "X_test = test_encoded[feature_order]\n",
    "\n",
    "# 5. 特徵標準化\n",
    "scaler = StandardScaler()\n",
    "X_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_features), \n",
    "    columns=X_features.columns, \n",
    "    index=X_features.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"資料前處理完成！\")\n",
    "print(f\"特徵數量: {len(feature_order)}\")\n",
    "print(f\"訓練集維度: {X_features_scaled.shape}\")\n",
    "print(f\"目標變數分布: {y_stroke.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8ae02",
   "metadata": {},
   "source": [
    "## 3. 資料分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff24ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集特徵維度: (3270, 37)\n",
      "驗證集特徵維度: (818, 37)\n",
      "訓練集標籤維度: (3270,)\n",
      "驗證集標籤維度: (818,)\n"
     ]
    }
   ],
   "source": [
    "# 將訓練資料分割為訓練集 (80%) 和驗證集 (20%)\n",
    "# 使用 stratify=y_stroke 確保訓練集和驗證集中的中風比例與原始資料相同，這對於不平衡資料集很重要\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_features_scaled,  # 使用標準化後的特徵\n",
    "    y_stroke, \n",
    "    test_size=0.2, \n",
    "    random_state=42,  # 統一使用固定的隨機種子\n",
    "    stratify=y_stroke\n",
    ")\n",
    "\n",
    "# 輸出分割後各資料集的維度和類別分布\n",
    "print(f\"訓練集特徵維度: {X_train.shape}\")\n",
    "print(f\"驗證集特徵維度: {X_val.shape}\")\n",
    "print(f\"訓練集標籤維度: {y_train.shape}\")\n",
    "print(f\"驗證集標籤維度: {y_val.shape}\")\n",
    "print(f\"\\n訓練集目標變數分布: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"驗證集目標變數分布: {y_val.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c630e",
   "metadata": {},
   "source": [
    "## 4. 建立 PyTorch 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8dd7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立自定義的 PyTorch Dataset\n",
    "class Custom_Generator(Dataset):\n",
    "    \"\"\"\n",
    "    自定義資料集類別，用於將 pandas DataFrame 轉換為 PyTorch Tensors。\n",
    "    \"\"\"\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # 將輸入資料轉換為 numpy array\n",
    "        self.X_data = X_data.values.astype(np.float32) if hasattr(X_data, 'values') else np.array(X_data, dtype=np.float32)\n",
    "        self.y_data = y_data.values.astype(np.int64) if hasattr(y_data, 'values') else np.array(y_data, dtype=np.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根據索引獲取一筆資料\n",
    "        features = self.X_data[index]\n",
    "        target = self.y_data[index]\n",
    "        \n",
    "        # 將資料轉換為 PyTorch Tensor\n",
    "        # 特徵使用 FloatTensor，因為模型權重和計算通常是浮點數\n",
    "        # 標籤使用 LongTensor，因為 CrossEntropyLoss 要求類別標籤為整數\n",
    "        return torch.FloatTensor(features), torch.LongTensor([target]).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        # 回傳資料集的總筆數\n",
    "        return len(self.X_data)\n",
    "\n",
    "# 建立訓練資料的 DataLoader\n",
    "# DataLoader 會將資料集打包成一批批 (batch) 的資料，並可在訓練時打亂順序 (shuffle=True)\n",
    "train_data = Custom_Generator(X_data=X_train, y_data=y_train)\n",
    "trainloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# 建立驗證資料的 DataLoader\n",
    "val_data = Custom_Generator(X_data=X_val, y_data=y_val)\n",
    "testloader = DataLoader(val_data, batch_size=32, shuffle=False) # 驗證時不需要打亂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7861ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 模型架構 ---\n",
      "DNN(\n",
      "  (fc1): Linear(in_features=37, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "\n",
      "==============================\n",
      "\n",
      "--- 模型參數摘要 ---\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,864\n",
      "           Dropout-2                  [-1, 128]               0\n",
      "            Linear-3                   [-1, 64]           8,256\n",
      "           Dropout-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 32]           2,080\n",
      "           Dropout-6                   [-1, 32]               0\n",
      "            Linear-7                    [-1, 2]              66\n",
      "================================================================\n",
      "Total params: 15,266\n",
      "Trainable params: 15,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 定義優化的深度神經網路 (DNN) 模型架構\n",
    "class ImprovedDNN(nn.Module):\n",
    "    def __init__(self, input_shape: int, dropout_rate: float = 0.3):\n",
    "        super(ImprovedDNN, self).__init__()\n",
    "        \n",
    "        # 定義網路層 - 使用更深的架構\n",
    "        self.fc1 = nn.Linear(input_shape, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # 批次標準化\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 2)  # 輸出層\n",
    "        \n",
    "        # 定義 Dropout 層\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 權重初始化\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"使用 Xavier 初始化權重\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定義前向傳播路徑\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)  # 輸出層不使用激活函數\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 初始化優化後的模型並移至指定裝置\n",
    "model = ImprovedDNN(input_shape=X_train.shape[1], dropout_rate=0.3).to(device)\n",
    "\n",
    "# 顯示模型架構\n",
    "print(\"--- 優化後的模型架構 ---\")\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 顯示模型參數摘要\n",
    "print(\"--- 模型參數摘要 ---\")\n",
    "summary(model, input_size=(X_train.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41ed77",
   "metadata": {},
   "source": [
    "## 5. 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "計算出的類別權重: tensor([ 0.5256, 10.2830])\n"
     ]
    }
   ],
   "source": [
    "# --- 設定優化的訓練參數 ---\n",
    "\n",
    "# 1. 處理類別不平衡：計算類別權重\n",
    "# 'balanced' 模式會自動計算權重，使得樣本數較少的類別獲得較高的權重\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"計算出的類別權重: {class_weights}\")\n",
    "\n",
    "# 2. 設定損失函數\n",
    "# 使用 CrossEntropyLoss，並傳入計算好的權重來處理不平衡問題\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 3. 設定優化器 - 使用 AdamW (改進版的 Adam)\n",
    "# AdamW 在 Adam 的基礎上改進了權重衰減的方式\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# 4. 設定學習率排程器 - 使用 ReduceLROnPlateau (根據驗證損失調整)\n",
    "# 當驗證損失在一定的 epoch 內沒有改善時，降低學習率\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# 5. 早停參數\n",
    "# 為了防止過擬合，當驗證損失在一定的 epoch 內沒有改善時，提前停止訓練\n",
    "patience = 20  # 驗證損失沒有改善的最大 epoch 數\n",
    "min_delta = 0.001  # 最小改善幅度\n",
    "\n",
    "# 6. 設定訓練週期 (Epochs)\n",
    "# 減少 epoch 數，依賴早停機制\n",
    "epochs = 200 # 為了示範，減少 epoch 數量，可根據實際情況調整\n",
    "\n",
    "# 7. 設定模型儲存路徑\n",
    "# 訓練完成後，將最佳模型儲存到指定的路徑\n",
    "folder = 'save_model'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "model_path = './save_model/best_improved_model.pth'\n",
    "\n",
    "print(f\"\\n訓練參數設定完成:\")\n",
    "print(f\"- 學習率: {learning_rate}\")\n",
    "print(f\"- 最大 epochs: {epochs}\")\n",
    "print(f\"- 早停耐心值: {patience}\")\n",
    "print(f\"- 模型儲存路徑: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35991eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 0.8943, Train Acc: 0.6624 | Valid Loss: 0.6494, Valid Acc: 0.5966 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.5966 ***\n",
      "Epoch [2/300] | Train Loss: 0.6164, Train Acc: 0.6844 | Valid Loss: 0.5209, Valid Acc: 0.7139 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.7139 ***\n",
      "Epoch [2/300] | Train Loss: 0.6164, Train Acc: 0.6844 | Valid Loss: 0.5209, Valid Acc: 0.7139 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.7139 ***\n",
      "Epoch [3/300] | Train Loss: 0.5860, Train Acc: 0.6838 | Valid Loss: 0.5093, Valid Acc: 0.7641 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.7641 ***\n",
      "Epoch [3/300] | Train Loss: 0.5860, Train Acc: 0.6838 | Valid Loss: 0.5093, Valid Acc: 0.7641 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.7641 ***\n",
      "Epoch [4/300] | Train Loss: 0.5524, Train Acc: 0.6936 | Valid Loss: 0.5136, Valid Acc: 0.6724 | LR: 0.001000\n",
      "Epoch [4/300] | Train Loss: 0.5524, Train Acc: 0.6936 | Valid Loss: 0.5136, Valid Acc: 0.6724 | LR: 0.001000\n",
      "Epoch [5/300] | Train Loss: 0.5682, Train Acc: 0.7095 | Valid Loss: 0.5530, Valid Acc: 0.8533 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.8533 ***\n",
      "Epoch [5/300] | Train Loss: 0.5682, Train Acc: 0.7095 | Valid Loss: 0.5530, Valid Acc: 0.8533 | LR: 0.001000\n",
      "*** Best model saved with accuracy: 0.8533 ***\n",
      "Epoch [6/300] | Train Loss: 0.5533, Train Acc: 0.7303 | Valid Loss: 0.5228, Valid Acc: 0.6663 | LR: 0.001000\n",
      "Epoch [6/300] | Train Loss: 0.5533, Train Acc: 0.7303 | Valid Loss: 0.5228, Valid Acc: 0.6663 | LR: 0.001000\n",
      "Epoch [7/300] | Train Loss: 0.5678, Train Acc: 0.7119 | Valid Loss: 0.5506, Valid Acc: 0.6516 | LR: 0.001000\n",
      "Epoch [7/300] | Train Loss: 0.5678, Train Acc: 0.7119 | Valid Loss: 0.5506, Valid Acc: 0.6516 | LR: 0.001000\n",
      "Epoch [8/300] | Train Loss: 0.5277, Train Acc: 0.7554 | Valid Loss: 0.4961, Valid Acc: 0.7115 | LR: 0.001000\n",
      "Epoch [8/300] | Train Loss: 0.5277, Train Acc: 0.7554 | Valid Loss: 0.4961, Valid Acc: 0.7115 | LR: 0.001000\n",
      "Epoch [9/300] | Train Loss: 0.5361, Train Acc: 0.7147 | Valid Loss: 0.4976, Valid Acc: 0.7017 | LR: 0.001000\n",
      "Epoch [9/300] | Train Loss: 0.5361, Train Acc: 0.7147 | Valid Loss: 0.4976, Valid Acc: 0.7017 | LR: 0.001000\n",
      "Epoch [10/300] | Train Loss: 0.5367, Train Acc: 0.7453 | Valid Loss: 0.5061, Valid Acc: 0.6748 | LR: 0.001000\n",
      "Epoch [10/300] | Train Loss: 0.5367, Train Acc: 0.7453 | Valid Loss: 0.5061, Valid Acc: 0.6748 | LR: 0.001000\n",
      "Epoch [11/300] | Train Loss: 0.5183, Train Acc: 0.7150 | Valid Loss: 0.5145, Valid Acc: 0.8447 | LR: 0.001000\n",
      "Epoch [11/300] | Train Loss: 0.5183, Train Acc: 0.7150 | Valid Loss: 0.5145, Valid Acc: 0.8447 | LR: 0.001000\n",
      "Epoch [12/300] | Train Loss: 0.5261, Train Acc: 0.7841 | Valid Loss: 0.4961, Valid Acc: 0.7897 | LR: 0.001000\n",
      "Epoch [12/300] | Train Loss: 0.5261, Train Acc: 0.7841 | Valid Loss: 0.4961, Valid Acc: 0.7897 | LR: 0.001000\n",
      "Epoch [13/300] | Train Loss: 0.5153, Train Acc: 0.7502 | Valid Loss: 0.4967, Valid Acc: 0.7848 | LR: 0.001000\n",
      "Epoch [13/300] | Train Loss: 0.5153, Train Acc: 0.7502 | Valid Loss: 0.4967, Valid Acc: 0.7848 | LR: 0.001000\n",
      "Epoch [14/300] | Train Loss: 0.4946, Train Acc: 0.7557 | Valid Loss: 0.4872, Valid Acc: 0.7445 | LR: 0.001000\n",
      "Epoch [14/300] | Train Loss: 0.4946, Train Acc: 0.7557 | Valid Loss: 0.4872, Valid Acc: 0.7445 | LR: 0.001000\n",
      "Epoch [15/300] | Train Loss: 0.4930, Train Acc: 0.7664 | Valid Loss: 0.4982, Valid Acc: 0.7359 | LR: 0.001000\n",
      "Epoch [15/300] | Train Loss: 0.4930, Train Acc: 0.7664 | Valid Loss: 0.4982, Valid Acc: 0.7359 | LR: 0.001000\n",
      "Epoch [16/300] | Train Loss: 0.4970, Train Acc: 0.7547 | Valid Loss: 0.4989, Valid Acc: 0.6760 | LR: 0.001000\n",
      "Epoch [16/300] | Train Loss: 0.4970, Train Acc: 0.7547 | Valid Loss: 0.4989, Valid Acc: 0.6760 | LR: 0.001000\n",
      "Epoch [17/300] | Train Loss: 0.5041, Train Acc: 0.7428 | Valid Loss: 0.5024, Valid Acc: 0.7359 | LR: 0.001000\n",
      "Epoch [17/300] | Train Loss: 0.5041, Train Acc: 0.7428 | Valid Loss: 0.5024, Valid Acc: 0.7359 | LR: 0.001000\n",
      "Epoch [18/300] | Train Loss: 0.4805, Train Acc: 0.7517 | Valid Loss: 0.4958, Valid Acc: 0.7738 | LR: 0.001000\n",
      "Epoch [18/300] | Train Loss: 0.4805, Train Acc: 0.7517 | Valid Loss: 0.4958, Valid Acc: 0.7738 | LR: 0.001000\n",
      "Epoch [19/300] | Train Loss: 0.5073, Train Acc: 0.7593 | Valid Loss: 0.4927, Valid Acc: 0.7836 | LR: 0.001000\n",
      "Epoch [19/300] | Train Loss: 0.5073, Train Acc: 0.7593 | Valid Loss: 0.4927, Valid Acc: 0.7836 | LR: 0.001000\n",
      "Epoch [20/300] | Train Loss: 0.4830, Train Acc: 0.7437 | Valid Loss: 0.5002, Valid Acc: 0.7738 | LR: 0.001000\n",
      "Epoch [20/300] | Train Loss: 0.4830, Train Acc: 0.7437 | Valid Loss: 0.5002, Valid Acc: 0.7738 | LR: 0.001000\n",
      "Epoch [21/300] | Train Loss: 0.4768, Train Acc: 0.7599 | Valid Loss: 0.4955, Valid Acc: 0.7494 | LR: 0.001000\n",
      "Epoch [21/300] | Train Loss: 0.4768, Train Acc: 0.7599 | Valid Loss: 0.4955, Valid Acc: 0.7494 | LR: 0.001000\n",
      "Epoch [22/300] | Train Loss: 0.4813, Train Acc: 0.7795 | Valid Loss: 0.4894, Valid Acc: 0.7421 | LR: 0.001000\n",
      "Epoch [22/300] | Train Loss: 0.4813, Train Acc: 0.7795 | Valid Loss: 0.4894, Valid Acc: 0.7421 | LR: 0.001000\n",
      "Epoch [23/300] | Train Loss: 0.4792, Train Acc: 0.7550 | Valid Loss: 0.4990, Valid Acc: 0.6650 | LR: 0.001000\n",
      "Epoch [23/300] | Train Loss: 0.4792, Train Acc: 0.7550 | Valid Loss: 0.4990, Valid Acc: 0.6650 | LR: 0.001000\n",
      "Epoch [24/300] | Train Loss: 0.4711, Train Acc: 0.7755 | Valid Loss: 0.5096, Valid Acc: 0.7029 | LR: 0.001000\n",
      "Epoch [24/300] | Train Loss: 0.4711, Train Acc: 0.7755 | Valid Loss: 0.5096, Valid Acc: 0.7029 | LR: 0.001000\n",
      "Epoch [25/300] | Train Loss: 0.4635, Train Acc: 0.7697 | Valid Loss: 0.4941, Valid Acc: 0.7335 | LR: 0.001000\n",
      "Epoch [25/300] | Train Loss: 0.4635, Train Acc: 0.7697 | Valid Loss: 0.4941, Valid Acc: 0.7335 | LR: 0.001000\n",
      "Epoch [26/300] | Train Loss: 0.4657, Train Acc: 0.7774 | Valid Loss: 0.5075, Valid Acc: 0.7971 | LR: 0.001000\n",
      "Epoch [26/300] | Train Loss: 0.4657, Train Acc: 0.7774 | Valid Loss: 0.5075, Valid Acc: 0.7971 | LR: 0.001000\n",
      "Epoch [27/300] | Train Loss: 0.4829, Train Acc: 0.7648 | Valid Loss: 0.5241, Valid Acc: 0.8007 | LR: 0.001000\n",
      "Epoch [27/300] | Train Loss: 0.4829, Train Acc: 0.7648 | Valid Loss: 0.5241, Valid Acc: 0.8007 | LR: 0.001000\n",
      "Epoch [28/300] | Train Loss: 0.4864, Train Acc: 0.7609 | Valid Loss: 0.5266, Valid Acc: 0.8350 | LR: 0.001000\n",
      "Epoch [28/300] | Train Loss: 0.4864, Train Acc: 0.7609 | Valid Loss: 0.5266, Valid Acc: 0.8350 | LR: 0.001000\n",
      "Epoch [29/300] | Train Loss: 0.4708, Train Acc: 0.7780 | Valid Loss: 0.4992, Valid Acc: 0.7604 | LR: 0.001000\n",
      "Epoch [29/300] | Train Loss: 0.4708, Train Acc: 0.7780 | Valid Loss: 0.4992, Valid Acc: 0.7604 | LR: 0.001000\n",
      "Epoch [30/300] | Train Loss: 0.4786, Train Acc: 0.7587 | Valid Loss: 0.5064, Valid Acc: 0.7506 | LR: 0.001000\n",
      "Epoch [30/300] | Train Loss: 0.4786, Train Acc: 0.7587 | Valid Loss: 0.5064, Valid Acc: 0.7506 | LR: 0.001000\n",
      "Epoch [31/300] | Train Loss: 0.4909, Train Acc: 0.7820 | Valid Loss: 0.5093, Valid Acc: 0.8166 | LR: 0.000500\n",
      "Epoch [31/300] | Train Loss: 0.4909, Train Acc: 0.7820 | Valid Loss: 0.5093, Valid Acc: 0.8166 | LR: 0.000500\n",
      "Epoch [32/300] | Train Loss: 0.4701, Train Acc: 0.7691 | Valid Loss: 0.5099, Valid Acc: 0.8020 | LR: 0.000500\n",
      "Epoch [32/300] | Train Loss: 0.4701, Train Acc: 0.7691 | Valid Loss: 0.5099, Valid Acc: 0.8020 | LR: 0.000500\n",
      "Epoch [33/300] | Train Loss: 0.4666, Train Acc: 0.7893 | Valid Loss: 0.5072, Valid Acc: 0.7604 | LR: 0.000500\n",
      "Epoch [33/300] | Train Loss: 0.4666, Train Acc: 0.7893 | Valid Loss: 0.5072, Valid Acc: 0.7604 | LR: 0.000500\n",
      "Epoch [34/300] | Train Loss: 0.4482, Train Acc: 0.7654 | Valid Loss: 0.5091, Valid Acc: 0.7787 | LR: 0.000500\n",
      "Epoch [34/300] | Train Loss: 0.4482, Train Acc: 0.7654 | Valid Loss: 0.5091, Valid Acc: 0.7787 | LR: 0.000500\n",
      "Epoch [35/300] | Train Loss: 0.4493, Train Acc: 0.7835 | Valid Loss: 0.5246, Valid Acc: 0.7934 | LR: 0.000500\n",
      "Epoch [35/300] | Train Loss: 0.4493, Train Acc: 0.7835 | Valid Loss: 0.5246, Valid Acc: 0.7934 | LR: 0.000500\n",
      "Epoch [36/300] | Train Loss: 0.4561, Train Acc: 0.7685 | Valid Loss: 0.5161, Valid Acc: 0.7702 | LR: 0.000500\n",
      "Epoch [36/300] | Train Loss: 0.4561, Train Acc: 0.7685 | Valid Loss: 0.5161, Valid Acc: 0.7702 | LR: 0.000500\n",
      "Epoch [37/300] | Train Loss: 0.4506, Train Acc: 0.7911 | Valid Loss: 0.5086, Valid Acc: 0.7665 | LR: 0.000500\n",
      "Epoch [37/300] | Train Loss: 0.4506, Train Acc: 0.7911 | Valid Loss: 0.5086, Valid Acc: 0.7665 | LR: 0.000500\n",
      "Epoch [38/300] | Train Loss: 0.4511, Train Acc: 0.7716 | Valid Loss: 0.5184, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [38/300] | Train Loss: 0.4511, Train Acc: 0.7716 | Valid Loss: 0.5184, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [39/300] | Train Loss: 0.4644, Train Acc: 0.7804 | Valid Loss: 0.5208, Valid Acc: 0.7555 | LR: 0.000500\n",
      "Epoch [39/300] | Train Loss: 0.4644, Train Acc: 0.7804 | Valid Loss: 0.5208, Valid Acc: 0.7555 | LR: 0.000500\n",
      "Epoch [40/300] | Train Loss: 0.4719, Train Acc: 0.7602 | Valid Loss: 0.5209, Valid Acc: 0.7726 | LR: 0.000500\n",
      "Epoch [40/300] | Train Loss: 0.4719, Train Acc: 0.7602 | Valid Loss: 0.5209, Valid Acc: 0.7726 | LR: 0.000500\n",
      "Epoch [41/300] | Train Loss: 0.4604, Train Acc: 0.7761 | Valid Loss: 0.5195, Valid Acc: 0.7763 | LR: 0.000500\n",
      "Epoch [41/300] | Train Loss: 0.4604, Train Acc: 0.7761 | Valid Loss: 0.5195, Valid Acc: 0.7763 | LR: 0.000500\n",
      "Epoch [42/300] | Train Loss: 0.4547, Train Acc: 0.7972 | Valid Loss: 0.5370, Valid Acc: 0.7897 | LR: 0.000500\n",
      "Epoch [42/300] | Train Loss: 0.4547, Train Acc: 0.7972 | Valid Loss: 0.5370, Valid Acc: 0.7897 | LR: 0.000500\n",
      "Epoch [43/300] | Train Loss: 0.4507, Train Acc: 0.7841 | Valid Loss: 0.5294, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [43/300] | Train Loss: 0.4507, Train Acc: 0.7841 | Valid Loss: 0.5294, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [44/300] | Train Loss: 0.4603, Train Acc: 0.7624 | Valid Loss: 0.5179, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [44/300] | Train Loss: 0.4603, Train Acc: 0.7624 | Valid Loss: 0.5179, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [45/300] | Train Loss: 0.4499, Train Acc: 0.7878 | Valid Loss: 0.5276, Valid Acc: 0.7800 | LR: 0.000500\n",
      "Epoch [45/300] | Train Loss: 0.4499, Train Acc: 0.7878 | Valid Loss: 0.5276, Valid Acc: 0.7800 | LR: 0.000500\n",
      "Epoch [46/300] | Train Loss: 0.4469, Train Acc: 0.7771 | Valid Loss: 0.5267, Valid Acc: 0.7506 | LR: 0.000500\n",
      "Epoch [46/300] | Train Loss: 0.4469, Train Acc: 0.7771 | Valid Loss: 0.5267, Valid Acc: 0.7506 | LR: 0.000500\n",
      "Epoch [47/300] | Train Loss: 0.4572, Train Acc: 0.7810 | Valid Loss: 0.5389, Valid Acc: 0.7873 | LR: 0.000500\n",
      "Epoch [47/300] | Train Loss: 0.4572, Train Acc: 0.7810 | Valid Loss: 0.5389, Valid Acc: 0.7873 | LR: 0.000500\n",
      "Epoch [48/300] | Train Loss: 0.4453, Train Acc: 0.7777 | Valid Loss: 0.5331, Valid Acc: 0.7775 | LR: 0.000500\n",
      "Epoch [48/300] | Train Loss: 0.4453, Train Acc: 0.7777 | Valid Loss: 0.5331, Valid Acc: 0.7775 | LR: 0.000500\n",
      "Epoch [49/300] | Train Loss: 0.4524, Train Acc: 0.7869 | Valid Loss: 0.5169, Valid Acc: 0.7714 | LR: 0.000500\n",
      "Epoch [49/300] | Train Loss: 0.4524, Train Acc: 0.7869 | Valid Loss: 0.5169, Valid Acc: 0.7714 | LR: 0.000500\n",
      "Epoch [50/300] | Train Loss: 0.4531, Train Acc: 0.7988 | Valid Loss: 0.5162, Valid Acc: 0.7372 | LR: 0.000500\n",
      "Epoch [50/300] | Train Loss: 0.4531, Train Acc: 0.7988 | Valid Loss: 0.5162, Valid Acc: 0.7372 | LR: 0.000500\n",
      "Epoch [51/300] | Train Loss: 0.4417, Train Acc: 0.7752 | Valid Loss: 0.5215, Valid Acc: 0.7885 | LR: 0.000500\n",
      "Epoch [51/300] | Train Loss: 0.4417, Train Acc: 0.7752 | Valid Loss: 0.5215, Valid Acc: 0.7885 | LR: 0.000500\n",
      "Epoch [52/300] | Train Loss: 0.4398, Train Acc: 0.7951 | Valid Loss: 0.5135, Valid Acc: 0.7641 | LR: 0.000500\n",
      "Epoch [52/300] | Train Loss: 0.4398, Train Acc: 0.7951 | Valid Loss: 0.5135, Valid Acc: 0.7641 | LR: 0.000500\n",
      "Epoch [53/300] | Train Loss: 0.4437, Train Acc: 0.7945 | Valid Loss: 0.5256, Valid Acc: 0.7738 | LR: 0.000500\n",
      "Epoch [53/300] | Train Loss: 0.4437, Train Acc: 0.7945 | Valid Loss: 0.5256, Valid Acc: 0.7738 | LR: 0.000500\n",
      "Epoch [54/300] | Train Loss: 0.4451, Train Acc: 0.7865 | Valid Loss: 0.5260, Valid Acc: 0.7641 | LR: 0.000500\n",
      "Epoch [54/300] | Train Loss: 0.4451, Train Acc: 0.7865 | Valid Loss: 0.5260, Valid Acc: 0.7641 | LR: 0.000500\n",
      "Epoch [55/300] | Train Loss: 0.4513, Train Acc: 0.7783 | Valid Loss: 0.5413, Valid Acc: 0.7971 | LR: 0.000500\n",
      "Epoch [55/300] | Train Loss: 0.4513, Train Acc: 0.7783 | Valid Loss: 0.5413, Valid Acc: 0.7971 | LR: 0.000500\n",
      "Epoch [56/300] | Train Loss: 0.4583, Train Acc: 0.7835 | Valid Loss: 0.5181, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [56/300] | Train Loss: 0.4583, Train Acc: 0.7835 | Valid Loss: 0.5181, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [57/300] | Train Loss: 0.4449, Train Acc: 0.7765 | Valid Loss: 0.5169, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [57/300] | Train Loss: 0.4449, Train Acc: 0.7765 | Valid Loss: 0.5169, Valid Acc: 0.7567 | LR: 0.000500\n",
      "Epoch [58/300] | Train Loss: 0.4402, Train Acc: 0.7740 | Valid Loss: 0.5363, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [58/300] | Train Loss: 0.4402, Train Acc: 0.7740 | Valid Loss: 0.5363, Valid Acc: 0.7836 | LR: 0.000500\n",
      "Epoch [59/300] | Train Loss: 0.4614, Train Acc: 0.7636 | Valid Loss: 0.5513, Valid Acc: 0.8130 | LR: 0.000500\n",
      "Epoch [59/300] | Train Loss: 0.4614, Train Acc: 0.7636 | Valid Loss: 0.5513, Valid Acc: 0.8130 | LR: 0.000500\n",
      "Epoch [60/300] | Train Loss: 0.4394, Train Acc: 0.7969 | Valid Loss: 0.5329, Valid Acc: 0.7445 | LR: 0.000500\n",
      "Epoch [60/300] | Train Loss: 0.4394, Train Acc: 0.7969 | Valid Loss: 0.5329, Valid Acc: 0.7445 | LR: 0.000500\n",
      "Epoch [61/300] | Train Loss: 0.4434, Train Acc: 0.7749 | Valid Loss: 0.5432, Valid Acc: 0.7702 | LR: 0.000250\n",
      "Epoch [61/300] | Train Loss: 0.4434, Train Acc: 0.7749 | Valid Loss: 0.5432, Valid Acc: 0.7702 | LR: 0.000250\n",
      "Epoch [62/300] | Train Loss: 0.4311, Train Acc: 0.7737 | Valid Loss: 0.5407, Valid Acc: 0.7604 | LR: 0.000250\n",
      "Epoch [62/300] | Train Loss: 0.4311, Train Acc: 0.7737 | Valid Loss: 0.5407, Valid Acc: 0.7604 | LR: 0.000250\n",
      "Epoch [63/300] | Train Loss: 0.4459, Train Acc: 0.7777 | Valid Loss: 0.5482, Valid Acc: 0.7800 | LR: 0.000250\n",
      "Epoch [63/300] | Train Loss: 0.4459, Train Acc: 0.7777 | Valid Loss: 0.5482, Valid Acc: 0.7800 | LR: 0.000250\n",
      "Epoch [64/300] | Train Loss: 0.4431, Train Acc: 0.8040 | Valid Loss: 0.5448, Valid Acc: 0.7751 | LR: 0.000250\n",
      "Epoch [64/300] | Train Loss: 0.4431, Train Acc: 0.8040 | Valid Loss: 0.5448, Valid Acc: 0.7751 | LR: 0.000250\n",
      "Epoch [65/300] | Train Loss: 0.4453, Train Acc: 0.7884 | Valid Loss: 0.5444, Valid Acc: 0.7714 | LR: 0.000250\n",
      "Epoch [65/300] | Train Loss: 0.4453, Train Acc: 0.7884 | Valid Loss: 0.5444, Valid Acc: 0.7714 | LR: 0.000250\n",
      "Epoch [66/300] | Train Loss: 0.4299, Train Acc: 0.7829 | Valid Loss: 0.5492, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [66/300] | Train Loss: 0.4299, Train Acc: 0.7829 | Valid Loss: 0.5492, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [67/300] | Train Loss: 0.4387, Train Acc: 0.7917 | Valid Loss: 0.5488, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [67/300] | Train Loss: 0.4387, Train Acc: 0.7917 | Valid Loss: 0.5488, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [68/300] | Train Loss: 0.4317, Train Acc: 0.7991 | Valid Loss: 0.5510, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [68/300] | Train Loss: 0.4317, Train Acc: 0.7991 | Valid Loss: 0.5510, Valid Acc: 0.7689 | LR: 0.000250\n",
      "Epoch [69/300] | Train Loss: 0.4365, Train Acc: 0.7920 | Valid Loss: 0.5490, Valid Acc: 0.7726 | LR: 0.000250\n",
      "Epoch [69/300] | Train Loss: 0.4365, Train Acc: 0.7920 | Valid Loss: 0.5490, Valid Acc: 0.7726 | LR: 0.000250\n",
      "Epoch [70/300] | Train Loss: 0.4412, Train Acc: 0.7865 | Valid Loss: 0.5605, Valid Acc: 0.7763 | LR: 0.000250\n",
      "Epoch [70/300] | Train Loss: 0.4412, Train Acc: 0.7865 | Valid Loss: 0.5605, Valid Acc: 0.7763 | LR: 0.000250\n",
      "Epoch [71/300] | Train Loss: 0.4414, Train Acc: 0.8028 | Valid Loss: 0.5514, Valid Acc: 0.7787 | LR: 0.000250\n",
      "Epoch [71/300] | Train Loss: 0.4414, Train Acc: 0.8028 | Valid Loss: 0.5514, Valid Acc: 0.7787 | LR: 0.000250\n",
      "Epoch [72/300] | Train Loss: 0.4353, Train Acc: 0.7920 | Valid Loss: 0.5577, Valid Acc: 0.7787 | LR: 0.000250\n",
      "Epoch [72/300] | Train Loss: 0.4353, Train Acc: 0.7920 | Valid Loss: 0.5577, Valid Acc: 0.7787 | LR: 0.000250\n",
      "Epoch [73/300] | Train Loss: 0.4298, Train Acc: 0.7758 | Valid Loss: 0.5842, Valid Acc: 0.7910 | LR: 0.000250\n",
      "Epoch [73/300] | Train Loss: 0.4298, Train Acc: 0.7758 | Valid Loss: 0.5842, Valid Acc: 0.7910 | LR: 0.000250\n",
      "Epoch [74/300] | Train Loss: 0.4298, Train Acc: 0.8034 | Valid Loss: 0.5688, Valid Acc: 0.7726 | LR: 0.000250\n",
      "Epoch [74/300] | Train Loss: 0.4298, Train Acc: 0.8034 | Valid Loss: 0.5688, Valid Acc: 0.7726 | LR: 0.000250\n",
      "Epoch [75/300] | Train Loss: 0.4434, Train Acc: 0.7621 | Valid Loss: 0.5535, Valid Acc: 0.7579 | LR: 0.000250\n",
      "Epoch [75/300] | Train Loss: 0.4434, Train Acc: 0.7621 | Valid Loss: 0.5535, Valid Acc: 0.7579 | LR: 0.000250\n",
      "Epoch [76/300] | Train Loss: 0.4282, Train Acc: 0.7896 | Valid Loss: 0.5684, Valid Acc: 0.7751 | LR: 0.000250\n",
      "Epoch [76/300] | Train Loss: 0.4282, Train Acc: 0.7896 | Valid Loss: 0.5684, Valid Acc: 0.7751 | LR: 0.000250\n",
      "Epoch [77/300] | Train Loss: 0.4306, Train Acc: 0.7774 | Valid Loss: 0.5490, Valid Acc: 0.7665 | LR: 0.000250\n",
      "Epoch [77/300] | Train Loss: 0.4306, Train Acc: 0.7774 | Valid Loss: 0.5490, Valid Acc: 0.7665 | LR: 0.000250\n",
      "Epoch [78/300] | Train Loss: 0.4313, Train Acc: 0.8012 | Valid Loss: 0.5542, Valid Acc: 0.7555 | LR: 0.000250\n",
      "Epoch [78/300] | Train Loss: 0.4313, Train Acc: 0.8012 | Valid Loss: 0.5542, Valid Acc: 0.7555 | LR: 0.000250\n",
      "Epoch [79/300] | Train Loss: 0.4379, Train Acc: 0.7709 | Valid Loss: 0.5552, Valid Acc: 0.7824 | LR: 0.000250\n",
      "Epoch [79/300] | Train Loss: 0.4379, Train Acc: 0.7709 | Valid Loss: 0.5552, Valid Acc: 0.7824 | LR: 0.000250\n",
      "Epoch [80/300] | Train Loss: 0.4265, Train Acc: 0.7917 | Valid Loss: 0.5737, Valid Acc: 0.7800 | LR: 0.000250\n",
      "Epoch [80/300] | Train Loss: 0.4265, Train Acc: 0.7917 | Valid Loss: 0.5737, Valid Acc: 0.7800 | LR: 0.000250\n",
      "Epoch [81/300] | Train Loss: 0.4284, Train Acc: 0.7991 | Valid Loss: 0.5667, Valid Acc: 0.7641 | LR: 0.000250\n",
      "Epoch [81/300] | Train Loss: 0.4284, Train Acc: 0.7991 | Valid Loss: 0.5667, Valid Acc: 0.7641 | LR: 0.000250\n",
      "Epoch [82/300] | Train Loss: 0.4215, Train Acc: 0.7881 | Valid Loss: 0.5664, Valid Acc: 0.7469 | LR: 0.000250\n",
      "Epoch [82/300] | Train Loss: 0.4215, Train Acc: 0.7881 | Valid Loss: 0.5664, Valid Acc: 0.7469 | LR: 0.000250\n",
      "Epoch [83/300] | Train Loss: 0.4325, Train Acc: 0.7761 | Valid Loss: 0.5707, Valid Acc: 0.7531 | LR: 0.000250\n",
      "Epoch [83/300] | Train Loss: 0.4325, Train Acc: 0.7761 | Valid Loss: 0.5707, Valid Acc: 0.7531 | LR: 0.000250\n",
      "Epoch [84/300] | Train Loss: 0.4294, Train Acc: 0.7841 | Valid Loss: 0.5774, Valid Acc: 0.7763 | LR: 0.000250\n",
      "Epoch [84/300] | Train Loss: 0.4294, Train Acc: 0.7841 | Valid Loss: 0.5774, Valid Acc: 0.7763 | LR: 0.000250\n",
      "Epoch [85/300] | Train Loss: 0.4448, Train Acc: 0.7957 | Valid Loss: 0.5691, Valid Acc: 0.7812 | LR: 0.000250\n",
      "Epoch [85/300] | Train Loss: 0.4448, Train Acc: 0.7957 | Valid Loss: 0.5691, Valid Acc: 0.7812 | LR: 0.000250\n",
      "Epoch [86/300] | Train Loss: 0.4293, Train Acc: 0.7905 | Valid Loss: 0.5850, Valid Acc: 0.7824 | LR: 0.000250\n",
      "Epoch [86/300] | Train Loss: 0.4293, Train Acc: 0.7905 | Valid Loss: 0.5850, Valid Acc: 0.7824 | LR: 0.000250\n",
      "Epoch [87/300] | Train Loss: 0.4199, Train Acc: 0.8064 | Valid Loss: 0.5715, Valid Acc: 0.7702 | LR: 0.000250\n",
      "Epoch [87/300] | Train Loss: 0.4199, Train Acc: 0.8064 | Valid Loss: 0.5715, Valid Acc: 0.7702 | LR: 0.000250\n",
      "Epoch [88/300] | Train Loss: 0.4215, Train Acc: 0.7768 | Valid Loss: 0.5914, Valid Acc: 0.7812 | LR: 0.000250\n",
      "Epoch [88/300] | Train Loss: 0.4215, Train Acc: 0.7768 | Valid Loss: 0.5914, Valid Acc: 0.7812 | LR: 0.000250\n",
      "Epoch [89/300] | Train Loss: 0.4237, Train Acc: 0.7924 | Valid Loss: 0.6129, Valid Acc: 0.7836 | LR: 0.000250\n",
      "Epoch [89/300] | Train Loss: 0.4237, Train Acc: 0.7924 | Valid Loss: 0.6129, Valid Acc: 0.7836 | LR: 0.000250\n",
      "Epoch [90/300] | Train Loss: 0.4209, Train Acc: 0.7988 | Valid Loss: 0.6181, Valid Acc: 0.7897 | LR: 0.000250\n",
      "Epoch [90/300] | Train Loss: 0.4209, Train Acc: 0.7988 | Valid Loss: 0.6181, Valid Acc: 0.7897 | LR: 0.000250\n",
      "Epoch [91/300] | Train Loss: 0.4259, Train Acc: 0.7991 | Valid Loss: 0.6002, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [91/300] | Train Loss: 0.4259, Train Acc: 0.7991 | Valid Loss: 0.6002, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [92/300] | Train Loss: 0.4283, Train Acc: 0.7957 | Valid Loss: 0.6076, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [92/300] | Train Loss: 0.4283, Train Acc: 0.7957 | Valid Loss: 0.6076, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [93/300] | Train Loss: 0.4196, Train Acc: 0.7994 | Valid Loss: 0.6215, Valid Acc: 0.7897 | LR: 0.000125\n",
      "Epoch [93/300] | Train Loss: 0.4196, Train Acc: 0.7994 | Valid Loss: 0.6215, Valid Acc: 0.7897 | LR: 0.000125\n",
      "Epoch [94/300] | Train Loss: 0.4306, Train Acc: 0.8028 | Valid Loss: 0.6034, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [94/300] | Train Loss: 0.4306, Train Acc: 0.8028 | Valid Loss: 0.6034, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [95/300] | Train Loss: 0.4389, Train Acc: 0.7838 | Valid Loss: 0.5979, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [95/300] | Train Loss: 0.4389, Train Acc: 0.7838 | Valid Loss: 0.5979, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [96/300] | Train Loss: 0.4197, Train Acc: 0.7969 | Valid Loss: 0.6032, Valid Acc: 0.7775 | LR: 0.000125\n",
      "Epoch [96/300] | Train Loss: 0.4197, Train Acc: 0.7969 | Valid Loss: 0.6032, Valid Acc: 0.7775 | LR: 0.000125\n",
      "Epoch [97/300] | Train Loss: 0.4149, Train Acc: 0.7954 | Valid Loss: 0.5991, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [97/300] | Train Loss: 0.4149, Train Acc: 0.7954 | Valid Loss: 0.5991, Valid Acc: 0.7677 | LR: 0.000125\n",
      "Epoch [98/300] | Train Loss: 0.4194, Train Acc: 0.7859 | Valid Loss: 0.5979, Valid Acc: 0.7702 | LR: 0.000125\n",
      "Epoch [98/300] | Train Loss: 0.4194, Train Acc: 0.7859 | Valid Loss: 0.5979, Valid Acc: 0.7702 | LR: 0.000125\n",
      "Epoch [99/300] | Train Loss: 0.4245, Train Acc: 0.7896 | Valid Loss: 0.6014, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [99/300] | Train Loss: 0.4245, Train Acc: 0.7896 | Valid Loss: 0.6014, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [100/300] | Train Loss: 0.4110, Train Acc: 0.7865 | Valid Loss: 0.6126, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [100/300] | Train Loss: 0.4110, Train Acc: 0.7865 | Valid Loss: 0.6126, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [101/300] | Train Loss: 0.4232, Train Acc: 0.7865 | Valid Loss: 0.6085, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [101/300] | Train Loss: 0.4232, Train Acc: 0.7865 | Valid Loss: 0.6085, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [102/300] | Train Loss: 0.4171, Train Acc: 0.7890 | Valid Loss: 0.6044, Valid Acc: 0.7653 | LR: 0.000125\n",
      "Epoch [102/300] | Train Loss: 0.4171, Train Acc: 0.7890 | Valid Loss: 0.6044, Valid Acc: 0.7653 | LR: 0.000125\n",
      "Epoch [103/300] | Train Loss: 0.4298, Train Acc: 0.7945 | Valid Loss: 0.6208, Valid Acc: 0.7824 | LR: 0.000125\n",
      "Epoch [103/300] | Train Loss: 0.4298, Train Acc: 0.7945 | Valid Loss: 0.6208, Valid Acc: 0.7824 | LR: 0.000125\n",
      "Epoch [104/300] | Train Loss: 0.4229, Train Acc: 0.7844 | Valid Loss: 0.6102, Valid Acc: 0.7726 | LR: 0.000125\n",
      "Epoch [104/300] | Train Loss: 0.4229, Train Acc: 0.7844 | Valid Loss: 0.6102, Valid Acc: 0.7726 | LR: 0.000125\n",
      "Epoch [105/300] | Train Loss: 0.4244, Train Acc: 0.7862 | Valid Loss: 0.6149, Valid Acc: 0.7702 | LR: 0.000125\n",
      "Epoch [105/300] | Train Loss: 0.4244, Train Acc: 0.7862 | Valid Loss: 0.6149, Valid Acc: 0.7702 | LR: 0.000125\n",
      "Epoch [106/300] | Train Loss: 0.4298, Train Acc: 0.7881 | Valid Loss: 0.6195, Valid Acc: 0.7861 | LR: 0.000125\n",
      "Epoch [106/300] | Train Loss: 0.4298, Train Acc: 0.7881 | Valid Loss: 0.6195, Valid Acc: 0.7861 | LR: 0.000125\n",
      "Epoch [107/300] | Train Loss: 0.4159, Train Acc: 0.7933 | Valid Loss: 0.6214, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [107/300] | Train Loss: 0.4159, Train Acc: 0.7933 | Valid Loss: 0.6214, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [108/300] | Train Loss: 0.4329, Train Acc: 0.7893 | Valid Loss: 0.6035, Valid Acc: 0.7494 | LR: 0.000125\n",
      "Epoch [108/300] | Train Loss: 0.4329, Train Acc: 0.7893 | Valid Loss: 0.6035, Valid Acc: 0.7494 | LR: 0.000125\n",
      "Epoch [109/300] | Train Loss: 0.4240, Train Acc: 0.7719 | Valid Loss: 0.6203, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [109/300] | Train Loss: 0.4240, Train Acc: 0.7719 | Valid Loss: 0.6203, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [110/300] | Train Loss: 0.4127, Train Acc: 0.7939 | Valid Loss: 0.6327, Valid Acc: 0.7775 | LR: 0.000125\n",
      "Epoch [110/300] | Train Loss: 0.4127, Train Acc: 0.7939 | Valid Loss: 0.6327, Valid Acc: 0.7775 | LR: 0.000125\n",
      "Epoch [111/300] | Train Loss: 0.4269, Train Acc: 0.7881 | Valid Loss: 0.6258, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [111/300] | Train Loss: 0.4269, Train Acc: 0.7881 | Valid Loss: 0.6258, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [112/300] | Train Loss: 0.4234, Train Acc: 0.7963 | Valid Loss: 0.6354, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [112/300] | Train Loss: 0.4234, Train Acc: 0.7963 | Valid Loss: 0.6354, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [113/300] | Train Loss: 0.4206, Train Acc: 0.7930 | Valid Loss: 0.6407, Valid Acc: 0.7824 | LR: 0.000125\n",
      "Epoch [113/300] | Train Loss: 0.4206, Train Acc: 0.7930 | Valid Loss: 0.6407, Valid Acc: 0.7824 | LR: 0.000125\n",
      "Epoch [114/300] | Train Loss: 0.4180, Train Acc: 0.7881 | Valid Loss: 0.6319, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [114/300] | Train Loss: 0.4180, Train Acc: 0.7881 | Valid Loss: 0.6319, Valid Acc: 0.7714 | LR: 0.000125\n",
      "Epoch [115/300] | Train Loss: 0.4126, Train Acc: 0.7972 | Valid Loss: 0.6328, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [115/300] | Train Loss: 0.4126, Train Acc: 0.7972 | Valid Loss: 0.6328, Valid Acc: 0.7738 | LR: 0.000125\n",
      "Epoch [116/300] | Train Loss: 0.4168, Train Acc: 0.7862 | Valid Loss: 0.6286, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [116/300] | Train Loss: 0.4168, Train Acc: 0.7862 | Valid Loss: 0.6286, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [117/300] | Train Loss: 0.4154, Train Acc: 0.7966 | Valid Loss: 0.6429, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [117/300] | Train Loss: 0.4154, Train Acc: 0.7966 | Valid Loss: 0.6429, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [118/300] | Train Loss: 0.4070, Train Acc: 0.7942 | Valid Loss: 0.6391, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [118/300] | Train Loss: 0.4070, Train Acc: 0.7942 | Valid Loss: 0.6391, Valid Acc: 0.7763 | LR: 0.000125\n",
      "Epoch [119/300] | Train Loss: 0.4197, Train Acc: 0.8031 | Valid Loss: 0.6461, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [119/300] | Train Loss: 0.4197, Train Acc: 0.8031 | Valid Loss: 0.6461, Valid Acc: 0.7787 | LR: 0.000125\n",
      "Epoch [120/300] | Train Loss: 0.4137, Train Acc: 0.7948 | Valid Loss: 0.6626, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [120/300] | Train Loss: 0.4137, Train Acc: 0.7948 | Valid Loss: 0.6626, Valid Acc: 0.7812 | LR: 0.000125\n",
      "Epoch [121/300] | Train Loss: 0.4115, Train Acc: 0.7991 | Valid Loss: 0.6602, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [121/300] | Train Loss: 0.4115, Train Acc: 0.7991 | Valid Loss: 0.6602, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [122/300] | Train Loss: 0.4079, Train Acc: 0.7997 | Valid Loss: 0.6586, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [122/300] | Train Loss: 0.4079, Train Acc: 0.7997 | Valid Loss: 0.6586, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [123/300] | Train Loss: 0.4151, Train Acc: 0.7948 | Valid Loss: 0.6610, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [123/300] | Train Loss: 0.4151, Train Acc: 0.7948 | Valid Loss: 0.6610, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [124/300] | Train Loss: 0.4074, Train Acc: 0.7911 | Valid Loss: 0.6623, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [124/300] | Train Loss: 0.4074, Train Acc: 0.7911 | Valid Loss: 0.6623, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [125/300] | Train Loss: 0.4202, Train Acc: 0.7948 | Valid Loss: 0.6653, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [125/300] | Train Loss: 0.4202, Train Acc: 0.7948 | Valid Loss: 0.6653, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [126/300] | Train Loss: 0.4013, Train Acc: 0.8000 | Valid Loss: 0.6703, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [126/300] | Train Loss: 0.4013, Train Acc: 0.8000 | Valid Loss: 0.6703, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [127/300] | Train Loss: 0.4037, Train Acc: 0.8003 | Valid Loss: 0.6639, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [127/300] | Train Loss: 0.4037, Train Acc: 0.8003 | Valid Loss: 0.6639, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [128/300] | Train Loss: 0.4020, Train Acc: 0.7945 | Valid Loss: 0.6652, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [128/300] | Train Loss: 0.4020, Train Acc: 0.7945 | Valid Loss: 0.6652, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [129/300] | Train Loss: 0.4183, Train Acc: 0.7905 | Valid Loss: 0.6567, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [129/300] | Train Loss: 0.4183, Train Acc: 0.7905 | Valid Loss: 0.6567, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [130/300] | Train Loss: 0.4144, Train Acc: 0.7899 | Valid Loss: 0.6663, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [130/300] | Train Loss: 0.4144, Train Acc: 0.7899 | Valid Loss: 0.6663, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [131/300] | Train Loss: 0.4078, Train Acc: 0.7917 | Valid Loss: 0.6661, Valid Acc: 0.7836 | LR: 0.000063\n",
      "Epoch [131/300] | Train Loss: 0.4078, Train Acc: 0.7917 | Valid Loss: 0.6661, Valid Acc: 0.7836 | LR: 0.000063\n",
      "Epoch [132/300] | Train Loss: 0.3921, Train Acc: 0.8018 | Valid Loss: 0.6770, Valid Acc: 0.7848 | LR: 0.000063\n",
      "Epoch [132/300] | Train Loss: 0.3921, Train Acc: 0.8018 | Valid Loss: 0.6770, Valid Acc: 0.7848 | LR: 0.000063\n",
      "Epoch [133/300] | Train Loss: 0.4098, Train Acc: 0.8015 | Valid Loss: 0.6768, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [133/300] | Train Loss: 0.4098, Train Acc: 0.8015 | Valid Loss: 0.6768, Valid Acc: 0.7824 | LR: 0.000063\n",
      "Epoch [134/300] | Train Loss: 0.4122, Train Acc: 0.7960 | Valid Loss: 0.6766, Valid Acc: 0.7848 | LR: 0.000063\n",
      "Epoch [134/300] | Train Loss: 0.4122, Train Acc: 0.7960 | Valid Loss: 0.6766, Valid Acc: 0.7848 | LR: 0.000063\n",
      "Epoch [135/300] | Train Loss: 0.4173, Train Acc: 0.7954 | Valid Loss: 0.6699, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [135/300] | Train Loss: 0.4173, Train Acc: 0.7954 | Valid Loss: 0.6699, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [136/300] | Train Loss: 0.3992, Train Acc: 0.7966 | Valid Loss: 0.6767, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [136/300] | Train Loss: 0.3992, Train Acc: 0.7966 | Valid Loss: 0.6767, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [137/300] | Train Loss: 0.4010, Train Acc: 0.7878 | Valid Loss: 0.6825, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [137/300] | Train Loss: 0.4010, Train Acc: 0.7878 | Valid Loss: 0.6825, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [138/300] | Train Loss: 0.4229, Train Acc: 0.7951 | Valid Loss: 0.6700, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [138/300] | Train Loss: 0.4229, Train Acc: 0.7951 | Valid Loss: 0.6700, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [139/300] | Train Loss: 0.4211, Train Acc: 0.7896 | Valid Loss: 0.6637, Valid Acc: 0.7738 | LR: 0.000063\n",
      "Epoch [139/300] | Train Loss: 0.4211, Train Acc: 0.7896 | Valid Loss: 0.6637, Valid Acc: 0.7738 | LR: 0.000063\n",
      "Epoch [140/300] | Train Loss: 0.4118, Train Acc: 0.7875 | Valid Loss: 0.6625, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [140/300] | Train Loss: 0.4118, Train Acc: 0.7875 | Valid Loss: 0.6625, Valid Acc: 0.7763 | LR: 0.000063\n",
      "Epoch [141/300] | Train Loss: 0.4175, Train Acc: 0.7914 | Valid Loss: 0.6666, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [141/300] | Train Loss: 0.4175, Train Acc: 0.7914 | Valid Loss: 0.6666, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [142/300] | Train Loss: 0.4041, Train Acc: 0.7936 | Valid Loss: 0.6780, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [142/300] | Train Loss: 0.4041, Train Acc: 0.7936 | Valid Loss: 0.6780, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [143/300] | Train Loss: 0.4089, Train Acc: 0.7920 | Valid Loss: 0.6821, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [143/300] | Train Loss: 0.4089, Train Acc: 0.7920 | Valid Loss: 0.6821, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [144/300] | Train Loss: 0.4119, Train Acc: 0.7951 | Valid Loss: 0.6846, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [144/300] | Train Loss: 0.4119, Train Acc: 0.7951 | Valid Loss: 0.6846, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [145/300] | Train Loss: 0.4032, Train Acc: 0.8037 | Valid Loss: 0.6956, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [145/300] | Train Loss: 0.4032, Train Acc: 0.8037 | Valid Loss: 0.6956, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [146/300] | Train Loss: 0.4000, Train Acc: 0.7988 | Valid Loss: 0.7064, Valid Acc: 0.7836 | LR: 0.000063\n",
      "Epoch [146/300] | Train Loss: 0.4000, Train Acc: 0.7988 | Valid Loss: 0.7064, Valid Acc: 0.7836 | LR: 0.000063\n",
      "Epoch [147/300] | Train Loss: 0.4138, Train Acc: 0.7963 | Valid Loss: 0.7006, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [147/300] | Train Loss: 0.4138, Train Acc: 0.7963 | Valid Loss: 0.7006, Valid Acc: 0.7800 | LR: 0.000063\n",
      "Epoch [148/300] | Train Loss: 0.4097, Train Acc: 0.7869 | Valid Loss: 0.7039, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [148/300] | Train Loss: 0.4097, Train Acc: 0.7869 | Valid Loss: 0.7039, Valid Acc: 0.7787 | LR: 0.000063\n",
      "Epoch [149/300] | Train Loss: 0.4055, Train Acc: 0.7951 | Valid Loss: 0.7007, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [149/300] | Train Loss: 0.4055, Train Acc: 0.7951 | Valid Loss: 0.7007, Valid Acc: 0.7812 | LR: 0.000063\n",
      "Epoch [150/300] | Train Loss: 0.4052, Train Acc: 0.7972 | Valid Loss: 0.6959, Valid Acc: 0.7751 | LR: 0.000063\n",
      "Epoch [150/300] | Train Loss: 0.4052, Train Acc: 0.7972 | Valid Loss: 0.6959, Valid Acc: 0.7751 | LR: 0.000063\n",
      "Epoch [151/300] | Train Loss: 0.4188, Train Acc: 0.7905 | Valid Loss: 0.6968, Valid Acc: 0.7775 | LR: 0.000031\n",
      "Epoch [151/300] | Train Loss: 0.4188, Train Acc: 0.7905 | Valid Loss: 0.6968, Valid Acc: 0.7775 | LR: 0.000031\n",
      "Epoch [152/300] | Train Loss: 0.4101, Train Acc: 0.7927 | Valid Loss: 0.6961, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [152/300] | Train Loss: 0.4101, Train Acc: 0.7927 | Valid Loss: 0.6961, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [153/300] | Train Loss: 0.4138, Train Acc: 0.7917 | Valid Loss: 0.7016, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [153/300] | Train Loss: 0.4138, Train Acc: 0.7917 | Valid Loss: 0.7016, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [154/300] | Train Loss: 0.4165, Train Acc: 0.7936 | Valid Loss: 0.6976, Valid Acc: 0.7775 | LR: 0.000031\n",
      "Epoch [154/300] | Train Loss: 0.4165, Train Acc: 0.7936 | Valid Loss: 0.6976, Valid Acc: 0.7775 | LR: 0.000031\n",
      "Epoch [155/300] | Train Loss: 0.4048, Train Acc: 0.7972 | Valid Loss: 0.7016, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [155/300] | Train Loss: 0.4048, Train Acc: 0.7972 | Valid Loss: 0.7016, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [156/300] | Train Loss: 0.3974, Train Acc: 0.7982 | Valid Loss: 0.7057, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [156/300] | Train Loss: 0.3974, Train Acc: 0.7982 | Valid Loss: 0.7057, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [157/300] | Train Loss: 0.4083, Train Acc: 0.7972 | Valid Loss: 0.7016, Valid Acc: 0.7812 | LR: 0.000031\n",
      "Epoch [157/300] | Train Loss: 0.4083, Train Acc: 0.7972 | Valid Loss: 0.7016, Valid Acc: 0.7812 | LR: 0.000031\n",
      "Epoch [158/300] | Train Loss: 0.4046, Train Acc: 0.7948 | Valid Loss: 0.6996, Valid Acc: 0.7787 | LR: 0.000031\n",
      "Epoch [158/300] | Train Loss: 0.4046, Train Acc: 0.7948 | Valid Loss: 0.6996, Valid Acc: 0.7787 | LR: 0.000031\n",
      "Epoch [159/300] | Train Loss: 0.4110, Train Acc: 0.7972 | Valid Loss: 0.7051, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [159/300] | Train Loss: 0.4110, Train Acc: 0.7972 | Valid Loss: 0.7051, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [160/300] | Train Loss: 0.4030, Train Acc: 0.8009 | Valid Loss: 0.7112, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [160/300] | Train Loss: 0.4030, Train Acc: 0.8009 | Valid Loss: 0.7112, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [161/300] | Train Loss: 0.4031, Train Acc: 0.7945 | Valid Loss: 0.7090, Valid Acc: 0.7787 | LR: 0.000031\n",
      "Epoch [161/300] | Train Loss: 0.4031, Train Acc: 0.7945 | Valid Loss: 0.7090, Valid Acc: 0.7787 | LR: 0.000031\n",
      "Epoch [162/300] | Train Loss: 0.4085, Train Acc: 0.7933 | Valid Loss: 0.7047, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [162/300] | Train Loss: 0.4085, Train Acc: 0.7933 | Valid Loss: 0.7047, Valid Acc: 0.7800 | LR: 0.000031\n",
      "Epoch [163/300] | Train Loss: 0.4031, Train Acc: 0.8009 | Valid Loss: 0.7156, Valid Acc: 0.7848 | LR: 0.000031\n",
      "Epoch [163/300] | Train Loss: 0.4031, Train Acc: 0.8009 | Valid Loss: 0.7156, Valid Acc: 0.7848 | LR: 0.000031\n",
      "Epoch [164/300] | Train Loss: 0.3981, Train Acc: 0.7979 | Valid Loss: 0.7095, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [164/300] | Train Loss: 0.3981, Train Acc: 0.7979 | Valid Loss: 0.7095, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [165/300] | Train Loss: 0.4033, Train Acc: 0.7982 | Valid Loss: 0.7117, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [165/300] | Train Loss: 0.4033, Train Acc: 0.7982 | Valid Loss: 0.7117, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [166/300] | Train Loss: 0.4000, Train Acc: 0.7957 | Valid Loss: 0.7186, Valid Acc: 0.7885 | LR: 0.000031\n",
      "Epoch [166/300] | Train Loss: 0.4000, Train Acc: 0.7957 | Valid Loss: 0.7186, Valid Acc: 0.7885 | LR: 0.000031\n",
      "Epoch [167/300] | Train Loss: 0.4107, Train Acc: 0.7979 | Valid Loss: 0.7155, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [167/300] | Train Loss: 0.4107, Train Acc: 0.7979 | Valid Loss: 0.7155, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [168/300] | Train Loss: 0.4085, Train Acc: 0.7976 | Valid Loss: 0.7065, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [168/300] | Train Loss: 0.4085, Train Acc: 0.7976 | Valid Loss: 0.7065, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [169/300] | Train Loss: 0.3986, Train Acc: 0.8031 | Valid Loss: 0.7157, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [169/300] | Train Loss: 0.3986, Train Acc: 0.8031 | Valid Loss: 0.7157, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [170/300] | Train Loss: 0.3955, Train Acc: 0.8021 | Valid Loss: 0.7215, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [170/300] | Train Loss: 0.3955, Train Acc: 0.8021 | Valid Loss: 0.7215, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [171/300] | Train Loss: 0.4193, Train Acc: 0.7976 | Valid Loss: 0.7109, Valid Acc: 0.7848 | LR: 0.000031\n",
      "Epoch [171/300] | Train Loss: 0.4193, Train Acc: 0.7976 | Valid Loss: 0.7109, Valid Acc: 0.7848 | LR: 0.000031\n",
      "Epoch [172/300] | Train Loss: 0.4014, Train Acc: 0.7988 | Valid Loss: 0.7143, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [172/300] | Train Loss: 0.4014, Train Acc: 0.7988 | Valid Loss: 0.7143, Valid Acc: 0.7861 | LR: 0.000031\n",
      "Epoch [173/300] | Train Loss: 0.3957, Train Acc: 0.7945 | Valid Loss: 0.7245, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [173/300] | Train Loss: 0.3957, Train Acc: 0.7945 | Valid Loss: 0.7245, Valid Acc: 0.7910 | LR: 0.000031\n",
      "Epoch [174/300] | Train Loss: 0.4160, Train Acc: 0.8031 | Valid Loss: 0.7190, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [174/300] | Train Loss: 0.4160, Train Acc: 0.8031 | Valid Loss: 0.7190, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [175/300] | Train Loss: 0.4114, Train Acc: 0.7942 | Valid Loss: 0.7144, Valid Acc: 0.7812 | LR: 0.000031\n",
      "Epoch [175/300] | Train Loss: 0.4114, Train Acc: 0.7942 | Valid Loss: 0.7144, Valid Acc: 0.7812 | LR: 0.000031\n",
      "Epoch [176/300] | Train Loss: 0.4119, Train Acc: 0.7985 | Valid Loss: 0.7125, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [176/300] | Train Loss: 0.4119, Train Acc: 0.7985 | Valid Loss: 0.7125, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [177/300] | Train Loss: 0.4007, Train Acc: 0.8052 | Valid Loss: 0.7202, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [177/300] | Train Loss: 0.4007, Train Acc: 0.8052 | Valid Loss: 0.7202, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [178/300] | Train Loss: 0.4006, Train Acc: 0.7957 | Valid Loss: 0.7190, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [178/300] | Train Loss: 0.4006, Train Acc: 0.7957 | Valid Loss: 0.7190, Valid Acc: 0.7824 | LR: 0.000031\n",
      "Epoch [179/300] | Train Loss: 0.4023, Train Acc: 0.7972 | Valid Loss: 0.7278, Valid Acc: 0.7873 | LR: 0.000031\n",
      "Epoch [179/300] | Train Loss: 0.4023, Train Acc: 0.7972 | Valid Loss: 0.7278, Valid Acc: 0.7873 | LR: 0.000031\n",
      "Epoch [180/300] | Train Loss: 0.4059, Train Acc: 0.7979 | Valid Loss: 0.7284, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [180/300] | Train Loss: 0.4059, Train Acc: 0.7979 | Valid Loss: 0.7284, Valid Acc: 0.7836 | LR: 0.000031\n",
      "Epoch [181/300] | Train Loss: 0.4094, Train Acc: 0.7991 | Valid Loss: 0.7254, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [181/300] | Train Loss: 0.4094, Train Acc: 0.7991 | Valid Loss: 0.7254, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [182/300] | Train Loss: 0.4015, Train Acc: 0.8015 | Valid Loss: 0.7227, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [182/300] | Train Loss: 0.4015, Train Acc: 0.8015 | Valid Loss: 0.7227, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [183/300] | Train Loss: 0.4087, Train Acc: 0.7963 | Valid Loss: 0.7234, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [183/300] | Train Loss: 0.4087, Train Acc: 0.7963 | Valid Loss: 0.7234, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [184/300] | Train Loss: 0.4076, Train Acc: 0.7899 | Valid Loss: 0.7205, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [184/300] | Train Loss: 0.4076, Train Acc: 0.7899 | Valid Loss: 0.7205, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [185/300] | Train Loss: 0.4087, Train Acc: 0.7927 | Valid Loss: 0.7206, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [185/300] | Train Loss: 0.4087, Train Acc: 0.7927 | Valid Loss: 0.7206, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [186/300] | Train Loss: 0.4043, Train Acc: 0.7963 | Valid Loss: 0.7247, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [186/300] | Train Loss: 0.4043, Train Acc: 0.7963 | Valid Loss: 0.7247, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [187/300] | Train Loss: 0.3921, Train Acc: 0.7972 | Valid Loss: 0.7273, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [187/300] | Train Loss: 0.3921, Train Acc: 0.7972 | Valid Loss: 0.7273, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [188/300] | Train Loss: 0.4058, Train Acc: 0.7988 | Valid Loss: 0.7278, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [188/300] | Train Loss: 0.4058, Train Acc: 0.7988 | Valid Loss: 0.7278, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [189/300] | Train Loss: 0.4140, Train Acc: 0.7997 | Valid Loss: 0.7291, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [189/300] | Train Loss: 0.4140, Train Acc: 0.7997 | Valid Loss: 0.7291, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [190/300] | Train Loss: 0.3950, Train Acc: 0.7957 | Valid Loss: 0.7273, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [190/300] | Train Loss: 0.3950, Train Acc: 0.7957 | Valid Loss: 0.7273, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [191/300] | Train Loss: 0.4023, Train Acc: 0.7957 | Valid Loss: 0.7313, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [191/300] | Train Loss: 0.4023, Train Acc: 0.7957 | Valid Loss: 0.7313, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [192/300] | Train Loss: 0.4042, Train Acc: 0.7976 | Valid Loss: 0.7328, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [192/300] | Train Loss: 0.4042, Train Acc: 0.7976 | Valid Loss: 0.7328, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [193/300] | Train Loss: 0.4096, Train Acc: 0.7954 | Valid Loss: 0.7313, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [193/300] | Train Loss: 0.4096, Train Acc: 0.7954 | Valid Loss: 0.7313, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [194/300] | Train Loss: 0.3978, Train Acc: 0.7945 | Valid Loss: 0.7346, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [194/300] | Train Loss: 0.3978, Train Acc: 0.7945 | Valid Loss: 0.7346, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [195/300] | Train Loss: 0.3941, Train Acc: 0.7985 | Valid Loss: 0.7368, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [195/300] | Train Loss: 0.3941, Train Acc: 0.7985 | Valid Loss: 0.7368, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [196/300] | Train Loss: 0.3916, Train Acc: 0.8028 | Valid Loss: 0.7419, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [196/300] | Train Loss: 0.3916, Train Acc: 0.8028 | Valid Loss: 0.7419, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [197/300] | Train Loss: 0.4117, Train Acc: 0.8000 | Valid Loss: 0.7381, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [197/300] | Train Loss: 0.4117, Train Acc: 0.8000 | Valid Loss: 0.7381, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [198/300] | Train Loss: 0.3935, Train Acc: 0.8003 | Valid Loss: 0.7443, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [198/300] | Train Loss: 0.3935, Train Acc: 0.8003 | Valid Loss: 0.7443, Valid Acc: 0.7836 | LR: 0.000016\n",
      "Epoch [199/300] | Train Loss: 0.4034, Train Acc: 0.7966 | Valid Loss: 0.7379, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [199/300] | Train Loss: 0.4034, Train Acc: 0.7966 | Valid Loss: 0.7379, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [200/300] | Train Loss: 0.4012, Train Acc: 0.7960 | Valid Loss: 0.7349, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [200/300] | Train Loss: 0.4012, Train Acc: 0.7960 | Valid Loss: 0.7349, Valid Acc: 0.7800 | LR: 0.000016\n",
      "Epoch [201/300] | Train Loss: 0.4097, Train Acc: 0.7939 | Valid Loss: 0.7318, Valid Acc: 0.7787 | LR: 0.000016\n",
      "Epoch [201/300] | Train Loss: 0.4097, Train Acc: 0.7939 | Valid Loss: 0.7318, Valid Acc: 0.7787 | LR: 0.000016\n",
      "Epoch [202/300] | Train Loss: 0.3953, Train Acc: 0.7954 | Valid Loss: 0.7361, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [202/300] | Train Loss: 0.3953, Train Acc: 0.7954 | Valid Loss: 0.7361, Valid Acc: 0.7812 | LR: 0.000016\n",
      "Epoch [203/300] | Train Loss: 0.3995, Train Acc: 0.7960 | Valid Loss: 0.7416, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [203/300] | Train Loss: 0.3995, Train Acc: 0.7960 | Valid Loss: 0.7416, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [204/300] | Train Loss: 0.3997, Train Acc: 0.7988 | Valid Loss: 0.7443, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [204/300] | Train Loss: 0.3997, Train Acc: 0.7988 | Valid Loss: 0.7443, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [205/300] | Train Loss: 0.3981, Train Acc: 0.7982 | Valid Loss: 0.7432, Valid Acc: 0.7861 | LR: 0.000016\n",
      "Epoch [205/300] | Train Loss: 0.3981, Train Acc: 0.7982 | Valid Loss: 0.7432, Valid Acc: 0.7861 | LR: 0.000016\n",
      "Epoch [206/300] | Train Loss: 0.4040, Train Acc: 0.7982 | Valid Loss: 0.7438, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [206/300] | Train Loss: 0.4040, Train Acc: 0.7982 | Valid Loss: 0.7438, Valid Acc: 0.7824 | LR: 0.000016\n",
      "Epoch [207/300] | Train Loss: 0.4015, Train Acc: 0.7957 | Valid Loss: 0.7463, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [207/300] | Train Loss: 0.4015, Train Acc: 0.7957 | Valid Loss: 0.7463, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [208/300] | Train Loss: 0.4110, Train Acc: 0.7939 | Valid Loss: 0.7430, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [208/300] | Train Loss: 0.4110, Train Acc: 0.7939 | Valid Loss: 0.7430, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [209/300] | Train Loss: 0.3911, Train Acc: 0.7972 | Valid Loss: 0.7418, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [209/300] | Train Loss: 0.3911, Train Acc: 0.7972 | Valid Loss: 0.7418, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [210/300] | Train Loss: 0.4117, Train Acc: 0.7991 | Valid Loss: 0.7434, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [210/300] | Train Loss: 0.4117, Train Acc: 0.7991 | Valid Loss: 0.7434, Valid Acc: 0.7848 | LR: 0.000016\n",
      "Epoch [211/300] | Train Loss: 0.4074, Train Acc: 0.7954 | Valid Loss: 0.7433, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [211/300] | Train Loss: 0.4074, Train Acc: 0.7954 | Valid Loss: 0.7433, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [212/300] | Train Loss: 0.4084, Train Acc: 0.7994 | Valid Loss: 0.7438, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [212/300] | Train Loss: 0.4084, Train Acc: 0.7994 | Valid Loss: 0.7438, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [213/300] | Train Loss: 0.4056, Train Acc: 0.8003 | Valid Loss: 0.7421, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [213/300] | Train Loss: 0.4056, Train Acc: 0.8003 | Valid Loss: 0.7421, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [214/300] | Train Loss: 0.4115, Train Acc: 0.7960 | Valid Loss: 0.7396, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [214/300] | Train Loss: 0.4115, Train Acc: 0.7960 | Valid Loss: 0.7396, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [215/300] | Train Loss: 0.4050, Train Acc: 0.7994 | Valid Loss: 0.7406, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [215/300] | Train Loss: 0.4050, Train Acc: 0.7994 | Valid Loss: 0.7406, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [216/300] | Train Loss: 0.3913, Train Acc: 0.7979 | Valid Loss: 0.7437, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [216/300] | Train Loss: 0.3913, Train Acc: 0.7979 | Valid Loss: 0.7437, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [217/300] | Train Loss: 0.4078, Train Acc: 0.7976 | Valid Loss: 0.7407, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [217/300] | Train Loss: 0.4078, Train Acc: 0.7976 | Valid Loss: 0.7407, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [218/300] | Train Loss: 0.3971, Train Acc: 0.7969 | Valid Loss: 0.7417, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [218/300] | Train Loss: 0.3971, Train Acc: 0.7969 | Valid Loss: 0.7417, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [219/300] | Train Loss: 0.4035, Train Acc: 0.7982 | Valid Loss: 0.7418, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [219/300] | Train Loss: 0.4035, Train Acc: 0.7982 | Valid Loss: 0.7418, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [220/300] | Train Loss: 0.3920, Train Acc: 0.7966 | Valid Loss: 0.7416, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [220/300] | Train Loss: 0.3920, Train Acc: 0.7966 | Valid Loss: 0.7416, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [221/300] | Train Loss: 0.3893, Train Acc: 0.7991 | Valid Loss: 0.7447, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [221/300] | Train Loss: 0.3893, Train Acc: 0.7991 | Valid Loss: 0.7447, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [222/300] | Train Loss: 0.3953, Train Acc: 0.7969 | Valid Loss: 0.7481, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [222/300] | Train Loss: 0.3953, Train Acc: 0.7969 | Valid Loss: 0.7481, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [223/300] | Train Loss: 0.4046, Train Acc: 0.7924 | Valid Loss: 0.7460, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [223/300] | Train Loss: 0.4046, Train Acc: 0.7924 | Valid Loss: 0.7460, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [224/300] | Train Loss: 0.3959, Train Acc: 0.8003 | Valid Loss: 0.7458, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [224/300] | Train Loss: 0.3959, Train Acc: 0.8003 | Valid Loss: 0.7458, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [225/300] | Train Loss: 0.4073, Train Acc: 0.7985 | Valid Loss: 0.7450, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [225/300] | Train Loss: 0.4073, Train Acc: 0.7985 | Valid Loss: 0.7450, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [226/300] | Train Loss: 0.3988, Train Acc: 0.8034 | Valid Loss: 0.7459, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [226/300] | Train Loss: 0.3988, Train Acc: 0.8034 | Valid Loss: 0.7459, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [227/300] | Train Loss: 0.4040, Train Acc: 0.8000 | Valid Loss: 0.7472, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [227/300] | Train Loss: 0.4040, Train Acc: 0.8000 | Valid Loss: 0.7472, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [228/300] | Train Loss: 0.4003, Train Acc: 0.8003 | Valid Loss: 0.7502, Valid Acc: 0.7885 | LR: 0.000008\n",
      "Epoch [228/300] | Train Loss: 0.4003, Train Acc: 0.8003 | Valid Loss: 0.7502, Valid Acc: 0.7885 | LR: 0.000008\n",
      "Epoch [229/300] | Train Loss: 0.3996, Train Acc: 0.7976 | Valid Loss: 0.7512, Valid Acc: 0.7885 | LR: 0.000008\n",
      "Epoch [229/300] | Train Loss: 0.3996, Train Acc: 0.7976 | Valid Loss: 0.7512, Valid Acc: 0.7885 | LR: 0.000008\n",
      "Epoch [230/300] | Train Loss: 0.3892, Train Acc: 0.8034 | Valid Loss: 0.7529, Valid Acc: 0.7897 | LR: 0.000008\n",
      "Epoch [230/300] | Train Loss: 0.3892, Train Acc: 0.8034 | Valid Loss: 0.7529, Valid Acc: 0.7897 | LR: 0.000008\n",
      "Epoch [231/300] | Train Loss: 0.4035, Train Acc: 0.7991 | Valid Loss: 0.7531, Valid Acc: 0.7873 | LR: 0.000008\n",
      "Epoch [231/300] | Train Loss: 0.4035, Train Acc: 0.7991 | Valid Loss: 0.7531, Valid Acc: 0.7873 | LR: 0.000008\n",
      "Epoch [232/300] | Train Loss: 0.3992, Train Acc: 0.7994 | Valid Loss: 0.7532, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [232/300] | Train Loss: 0.3992, Train Acc: 0.7994 | Valid Loss: 0.7532, Valid Acc: 0.7861 | LR: 0.000008\n",
      "Epoch [233/300] | Train Loss: 0.3890, Train Acc: 0.7960 | Valid Loss: 0.7528, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [233/300] | Train Loss: 0.3890, Train Acc: 0.7960 | Valid Loss: 0.7528, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [234/300] | Train Loss: 0.3969, Train Acc: 0.8018 | Valid Loss: 0.7515, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [234/300] | Train Loss: 0.3969, Train Acc: 0.8018 | Valid Loss: 0.7515, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [235/300] | Train Loss: 0.4101, Train Acc: 0.7960 | Valid Loss: 0.7490, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [235/300] | Train Loss: 0.4101, Train Acc: 0.7960 | Valid Loss: 0.7490, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [236/300] | Train Loss: 0.3921, Train Acc: 0.7917 | Valid Loss: 0.7507, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [236/300] | Train Loss: 0.3921, Train Acc: 0.7917 | Valid Loss: 0.7507, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [237/300] | Train Loss: 0.3905, Train Acc: 0.7957 | Valid Loss: 0.7520, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [237/300] | Train Loss: 0.3905, Train Acc: 0.7957 | Valid Loss: 0.7520, Valid Acc: 0.7824 | LR: 0.000008\n",
      "Epoch [238/300] | Train Loss: 0.3945, Train Acc: 0.8018 | Valid Loss: 0.7540, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [238/300] | Train Loss: 0.3945, Train Acc: 0.8018 | Valid Loss: 0.7540, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [239/300] | Train Loss: 0.3982, Train Acc: 0.8000 | Valid Loss: 0.7547, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [239/300] | Train Loss: 0.3982, Train Acc: 0.8000 | Valid Loss: 0.7547, Valid Acc: 0.7848 | LR: 0.000008\n",
      "Epoch [240/300] | Train Loss: 0.4074, Train Acc: 0.7988 | Valid Loss: 0.7537, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [240/300] | Train Loss: 0.4074, Train Acc: 0.7988 | Valid Loss: 0.7537, Valid Acc: 0.7836 | LR: 0.000008\n",
      "Epoch [241/300] | Train Loss: 0.3999, Train Acc: 0.7966 | Valid Loss: 0.7532, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [241/300] | Train Loss: 0.3999, Train Acc: 0.7966 | Valid Loss: 0.7532, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [242/300] | Train Loss: 0.3981, Train Acc: 0.7969 | Valid Loss: 0.7526, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [242/300] | Train Loss: 0.3981, Train Acc: 0.7969 | Valid Loss: 0.7526, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [243/300] | Train Loss: 0.3953, Train Acc: 0.8003 | Valid Loss: 0.7527, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [243/300] | Train Loss: 0.3953, Train Acc: 0.8003 | Valid Loss: 0.7527, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [244/300] | Train Loss: 0.4024, Train Acc: 0.7994 | Valid Loss: 0.7533, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [244/300] | Train Loss: 0.4024, Train Acc: 0.7994 | Valid Loss: 0.7533, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [245/300] | Train Loss: 0.4118, Train Acc: 0.7991 | Valid Loss: 0.7527, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [245/300] | Train Loss: 0.4118, Train Acc: 0.7991 | Valid Loss: 0.7527, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [246/300] | Train Loss: 0.3866, Train Acc: 0.7957 | Valid Loss: 0.7538, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [246/300] | Train Loss: 0.3866, Train Acc: 0.7957 | Valid Loss: 0.7538, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [247/300] | Train Loss: 0.3925, Train Acc: 0.7972 | Valid Loss: 0.7539, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [247/300] | Train Loss: 0.3925, Train Acc: 0.7972 | Valid Loss: 0.7539, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [248/300] | Train Loss: 0.3971, Train Acc: 0.8021 | Valid Loss: 0.7544, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [248/300] | Train Loss: 0.3971, Train Acc: 0.8021 | Valid Loss: 0.7544, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [249/300] | Train Loss: 0.3993, Train Acc: 0.7979 | Valid Loss: 0.7544, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [249/300] | Train Loss: 0.3993, Train Acc: 0.7979 | Valid Loss: 0.7544, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [250/300] | Train Loss: 0.4015, Train Acc: 0.7969 | Valid Loss: 0.7551, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [250/300] | Train Loss: 0.4015, Train Acc: 0.7969 | Valid Loss: 0.7551, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [251/300] | Train Loss: 0.4069, Train Acc: 0.8018 | Valid Loss: 0.7537, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [251/300] | Train Loss: 0.4069, Train Acc: 0.8018 | Valid Loss: 0.7537, Valid Acc: 0.7836 | LR: 0.000004\n",
      "Epoch [252/300] | Train Loss: 0.4062, Train Acc: 0.7969 | Valid Loss: 0.7530, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [252/300] | Train Loss: 0.4062, Train Acc: 0.7969 | Valid Loss: 0.7530, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [253/300] | Train Loss: 0.4087, Train Acc: 0.7979 | Valid Loss: 0.7524, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [253/300] | Train Loss: 0.4087, Train Acc: 0.7979 | Valid Loss: 0.7524, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [254/300] | Train Loss: 0.3930, Train Acc: 0.7976 | Valid Loss: 0.7533, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [254/300] | Train Loss: 0.3930, Train Acc: 0.7976 | Valid Loss: 0.7533, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [255/300] | Train Loss: 0.3894, Train Acc: 0.7997 | Valid Loss: 0.7543, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [255/300] | Train Loss: 0.3894, Train Acc: 0.7997 | Valid Loss: 0.7543, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [256/300] | Train Loss: 0.3952, Train Acc: 0.7988 | Valid Loss: 0.7538, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [256/300] | Train Loss: 0.3952, Train Acc: 0.7988 | Valid Loss: 0.7538, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [257/300] | Train Loss: 0.3950, Train Acc: 0.8034 | Valid Loss: 0.7540, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [257/300] | Train Loss: 0.3950, Train Acc: 0.8034 | Valid Loss: 0.7540, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [258/300] | Train Loss: 0.3948, Train Acc: 0.7997 | Valid Loss: 0.7542, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [258/300] | Train Loss: 0.3948, Train Acc: 0.7997 | Valid Loss: 0.7542, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [259/300] | Train Loss: 0.4048, Train Acc: 0.7951 | Valid Loss: 0.7536, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [259/300] | Train Loss: 0.4048, Train Acc: 0.7951 | Valid Loss: 0.7536, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [260/300] | Train Loss: 0.4034, Train Acc: 0.7963 | Valid Loss: 0.7540, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [260/300] | Train Loss: 0.4034, Train Acc: 0.7963 | Valid Loss: 0.7540, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [261/300] | Train Loss: 0.4012, Train Acc: 0.7936 | Valid Loss: 0.7542, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [261/300] | Train Loss: 0.4012, Train Acc: 0.7936 | Valid Loss: 0.7542, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [262/300] | Train Loss: 0.4066, Train Acc: 0.7972 | Valid Loss: 0.7533, Valid Acc: 0.7812 | LR: 0.000004\n",
      "Epoch [262/300] | Train Loss: 0.4066, Train Acc: 0.7972 | Valid Loss: 0.7533, Valid Acc: 0.7812 | LR: 0.000004\n",
      "Epoch [263/300] | Train Loss: 0.4106, Train Acc: 0.7936 | Valid Loss: 0.7530, Valid Acc: 0.7812 | LR: 0.000004\n",
      "Epoch [263/300] | Train Loss: 0.4106, Train Acc: 0.7936 | Valid Loss: 0.7530, Valid Acc: 0.7812 | LR: 0.000004\n",
      "Epoch [264/300] | Train Loss: 0.3884, Train Acc: 0.7948 | Valid Loss: 0.7532, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [264/300] | Train Loss: 0.3884, Train Acc: 0.7948 | Valid Loss: 0.7532, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [265/300] | Train Loss: 0.4039, Train Acc: 0.7942 | Valid Loss: 0.7534, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [265/300] | Train Loss: 0.4039, Train Acc: 0.7942 | Valid Loss: 0.7534, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [266/300] | Train Loss: 0.3978, Train Acc: 0.7991 | Valid Loss: 0.7544, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [266/300] | Train Loss: 0.3978, Train Acc: 0.7991 | Valid Loss: 0.7544, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [267/300] | Train Loss: 0.4002, Train Acc: 0.7957 | Valid Loss: 0.7546, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [267/300] | Train Loss: 0.4002, Train Acc: 0.7957 | Valid Loss: 0.7546, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [268/300] | Train Loss: 0.3945, Train Acc: 0.7976 | Valid Loss: 0.7556, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [268/300] | Train Loss: 0.3945, Train Acc: 0.7976 | Valid Loss: 0.7556, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [269/300] | Train Loss: 0.3971, Train Acc: 0.7963 | Valid Loss: 0.7559, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [269/300] | Train Loss: 0.3971, Train Acc: 0.7963 | Valid Loss: 0.7559, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [270/300] | Train Loss: 0.3997, Train Acc: 0.7982 | Valid Loss: 0.7557, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [270/300] | Train Loss: 0.3997, Train Acc: 0.7982 | Valid Loss: 0.7557, Valid Acc: 0.7824 | LR: 0.000004\n",
      "Epoch [271/300] | Train Loss: 0.4056, Train Acc: 0.7985 | Valid Loss: 0.7556, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [271/300] | Train Loss: 0.4056, Train Acc: 0.7985 | Valid Loss: 0.7556, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [272/300] | Train Loss: 0.3968, Train Acc: 0.7966 | Valid Loss: 0.7552, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [272/300] | Train Loss: 0.3968, Train Acc: 0.7966 | Valid Loss: 0.7552, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [273/300] | Train Loss: 0.4034, Train Acc: 0.7957 | Valid Loss: 0.7551, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [273/300] | Train Loss: 0.4034, Train Acc: 0.7957 | Valid Loss: 0.7551, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [274/300] | Train Loss: 0.4015, Train Acc: 0.7930 | Valid Loss: 0.7549, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [274/300] | Train Loss: 0.4015, Train Acc: 0.7930 | Valid Loss: 0.7549, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [275/300] | Train Loss: 0.3957, Train Acc: 0.7994 | Valid Loss: 0.7550, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [275/300] | Train Loss: 0.3957, Train Acc: 0.7994 | Valid Loss: 0.7550, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [276/300] | Train Loss: 0.3998, Train Acc: 0.7957 | Valid Loss: 0.7552, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [276/300] | Train Loss: 0.3998, Train Acc: 0.7957 | Valid Loss: 0.7552, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [277/300] | Train Loss: 0.3995, Train Acc: 0.7957 | Valid Loss: 0.7555, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [277/300] | Train Loss: 0.3995, Train Acc: 0.7957 | Valid Loss: 0.7555, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [278/300] | Train Loss: 0.3992, Train Acc: 0.7945 | Valid Loss: 0.7558, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [278/300] | Train Loss: 0.3992, Train Acc: 0.7945 | Valid Loss: 0.7558, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [279/300] | Train Loss: 0.3949, Train Acc: 0.7982 | Valid Loss: 0.7560, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [279/300] | Train Loss: 0.3949, Train Acc: 0.7982 | Valid Loss: 0.7560, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [280/300] | Train Loss: 0.3959, Train Acc: 0.7969 | Valid Loss: 0.7570, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [280/300] | Train Loss: 0.3959, Train Acc: 0.7969 | Valid Loss: 0.7570, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [281/300] | Train Loss: 0.3859, Train Acc: 0.7976 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [281/300] | Train Loss: 0.3859, Train Acc: 0.7976 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [282/300] | Train Loss: 0.4067, Train Acc: 0.7966 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [282/300] | Train Loss: 0.4067, Train Acc: 0.7966 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [283/300] | Train Loss: 0.4006, Train Acc: 0.7988 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [283/300] | Train Loss: 0.4006, Train Acc: 0.7988 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [284/300] | Train Loss: 0.3960, Train Acc: 0.7966 | Valid Loss: 0.7565, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [284/300] | Train Loss: 0.3960, Train Acc: 0.7966 | Valid Loss: 0.7565, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [285/300] | Train Loss: 0.4048, Train Acc: 0.7939 | Valid Loss: 0.7560, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [285/300] | Train Loss: 0.4048, Train Acc: 0.7939 | Valid Loss: 0.7560, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [286/300] | Train Loss: 0.3927, Train Acc: 0.7951 | Valid Loss: 0.7567, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [286/300] | Train Loss: 0.3927, Train Acc: 0.7951 | Valid Loss: 0.7567, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [287/300] | Train Loss: 0.4046, Train Acc: 0.7914 | Valid Loss: 0.7567, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [287/300] | Train Loss: 0.4046, Train Acc: 0.7914 | Valid Loss: 0.7567, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [288/300] | Train Loss: 0.3948, Train Acc: 0.7991 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [288/300] | Train Loss: 0.3948, Train Acc: 0.7991 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [289/300] | Train Loss: 0.3993, Train Acc: 0.7979 | Valid Loss: 0.7574, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [289/300] | Train Loss: 0.3993, Train Acc: 0.7979 | Valid Loss: 0.7574, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [290/300] | Train Loss: 0.3926, Train Acc: 0.7994 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [290/300] | Train Loss: 0.3926, Train Acc: 0.7994 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [291/300] | Train Loss: 0.4013, Train Acc: 0.7966 | Valid Loss: 0.7574, Valid Acc: 0.7836 | LR: 0.000002\n",
      "Epoch [291/300] | Train Loss: 0.4013, Train Acc: 0.7966 | Valid Loss: 0.7574, Valid Acc: 0.7836 | LR: 0.000002\n",
      "Epoch [292/300] | Train Loss: 0.4057, Train Acc: 0.7972 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [292/300] | Train Loss: 0.4057, Train Acc: 0.7972 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [293/300] | Train Loss: 0.4100, Train Acc: 0.7948 | Valid Loss: 0.7563, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [293/300] | Train Loss: 0.4100, Train Acc: 0.7948 | Valid Loss: 0.7563, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [294/300] | Train Loss: 0.3972, Train Acc: 0.7997 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [294/300] | Train Loss: 0.3972, Train Acc: 0.7997 | Valid Loss: 0.7568, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [295/300] | Train Loss: 0.4014, Train Acc: 0.7948 | Valid Loss: 0.7569, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [295/300] | Train Loss: 0.4014, Train Acc: 0.7948 | Valid Loss: 0.7569, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [296/300] | Train Loss: 0.4042, Train Acc: 0.7985 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [296/300] | Train Loss: 0.4042, Train Acc: 0.7985 | Valid Loss: 0.7572, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [297/300] | Train Loss: 0.4019, Train Acc: 0.7979 | Valid Loss: 0.7570, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [297/300] | Train Loss: 0.4019, Train Acc: 0.7979 | Valid Loss: 0.7570, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [298/300] | Train Loss: 0.3971, Train Acc: 0.7988 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [298/300] | Train Loss: 0.3971, Train Acc: 0.7988 | Valid Loss: 0.7571, Valid Acc: 0.7824 | LR: 0.000002\n",
      "Epoch [299/300] | Train Loss: 0.4000, Train Acc: 0.7985 | Valid Loss: 0.7575, Valid Acc: 0.7836 | LR: 0.000002\n",
      "Epoch [299/300] | Train Loss: 0.4000, Train Acc: 0.7985 | Valid Loss: 0.7575, Valid Acc: 0.7836 | LR: 0.000002\n",
      "Epoch [300/300] | Train Loss: 0.3940, Train Acc: 0.7963 | Valid Loss: 0.7575, Valid Acc: 0.7836 | LR: 0.000002\n",
      "\n",
      "訓練完成！\n",
      "Epoch [300/300] | Train Loss: 0.3940, Train Acc: 0.7963 | Valid Loss: 0.7575, Valid Acc: 0.7836 | LR: 0.000002\n",
      "\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# 定義優化的訓練與驗證流程函式\n",
    "def improved_train_process(model, epochs: int, optimizer, criterion, scheduler,\n",
    "                          trainloader, testloader, model_path: str, \n",
    "                          patience: int = 20, min_delta: float = 0.001) -> Dict:\n",
    "    \"\"\"\n",
    "    優化的訓練流程，包含早停機制和更好的監控。\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch 模型\n",
    "        epochs: 最大訓練 epochs\n",
    "        optimizer: 優化器\n",
    "        criterion: 損失函數\n",
    "        scheduler: 學習率排程器\n",
    "        trainloader: 訓練資料載入器\n",
    "        testloader: 驗證資料載入器\n",
    "        model_path: 模型儲存路徑\n",
    "        patience: 早停耐心值\n",
    "        min_delta: 最小改善幅度\n",
    "        \n",
    "    Returns:\n",
    "        包含訓練歷史的字典\n",
    "    \"\"\"\n",
    "    # 用於記錄訓練過程的指標\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'valid_loss': [], 'valid_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    print(\"開始訓練...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            # --- 訓練階段 ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # 1. 梯度歸零\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 2. 前向傳播\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 3. 計算損失\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # 4. 反向傳播\n",
    "                loss.backward()\n",
    "                \n",
    "                # 5. 梯度裁剪 (防止梯度爆炸)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # 6. 更新權重\n",
    "                optimizer.step()\n",
    "\n",
    "                # 累加損失和準確率\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            # 計算平均訓練損失和準確率\n",
    "            epoch_train_loss = running_loss / len(trainloader)\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "            history['train_loss'].append(epoch_train_loss)\n",
    "            history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "            # --- 驗證階段 ---\n",
    "            model.eval()\n",
    "            valid_running_loss = 0.0\n",
    "            correct_valid = 0\n",
    "            total_valid = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    valid_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_valid += labels.size(0)\n",
    "                    correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "            # 計算平均驗證損失和準確率\n",
    "            epoch_valid_loss = valid_running_loss / len(testloader)\n",
    "            epoch_valid_acc = correct_valid / total_valid\n",
    "            history['valid_loss'].append(epoch_valid_loss)\n",
    "            history['valid_acc'].append(epoch_valid_acc)\n",
    "            \n",
    "            # 記錄當前學習率\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['lr'].append(current_lr)\n",
    "\n",
    "            # 更新學習率排程器\n",
    "            scheduler.step(epoch_valid_loss)\n",
    "            \n",
    "            # 輸出當前 epoch 的結果\n",
    "            print(f'Epoch [{epoch+1:3d}/{epochs}] | '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | '\n",
    "                  f'Valid Loss: {epoch_valid_loss:.4f}, Valid Acc: {epoch_valid_acc:.4f} | '\n",
    "                  f'LR: {current_lr:.6f}')\n",
    "\n",
    "            # 早停檢查\n",
    "            if epoch_valid_loss < best_valid_loss - min_delta:\n",
    "                best_valid_loss = epoch_valid_loss\n",
    "                epochs_without_improvement = 0\n",
    "                \n",
    "                # 如果驗證準確率也是最佳，儲存模型\n",
    "                if epoch_valid_acc > best_acc:\n",
    "                    best_acc = epoch_valid_acc\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_acc': best_acc,\n",
    "                        'best_loss': best_valid_loss\n",
    "                    }, model_path)\n",
    "                    print(f'*** 最佳模型已儲存 (Acc: {best_acc:.4f}, Loss: {best_valid_loss:.4f}) ***')\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            # 早停檢查\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\n早停觸發：驗證損失在 {patience} 個 epochs 內沒有改善\")\n",
    "                print(f\"最佳驗證損失: {best_valid_loss:.4f}\")\n",
    "                print(f\"最佳驗證準確率: {best_acc:.4f}\")\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n訓練被使用者中斷\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n訓練過程中發生錯誤: {e}\")\n",
    "        raise\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"訓練完成！\")\n",
    "    print(f\"最終最佳準確率: {best_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "# --- 執行優化的訓練 ---\n",
    "history = improved_train_process(\n",
    "    model=model, \n",
    "    epochs=epochs, \n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    trainloader=trainloader, \n",
    "    testloader=testloader, \n",
    "    model_path=model_path,\n",
    "    patience=patience,\n",
    "    min_delta=min_delta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0814d5",
   "metadata": {},
   "source": [
    "## 6. 結果評估與視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb289155",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 繪製 Training loss 和 Validation loss\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.subplot(\u001b[32m131\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mTrain_loss\u001b[49m)), Train_loss, label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(valid_loss)), valid_loss, label=\u001b[33m'\u001b[39m\u001b[33mTesting Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m plt.legend(loc=\u001b[33m'\u001b[39m\u001b[33mbest\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Train_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 優化的訓練過程視覺化\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# 繪製 Loss 曲線\n",
    "ax1.plot(history['train_loss'], label='Training Loss', linewidth=2, alpha=0.8)\n",
    "ax1.plot(history['valid_loss'], label='Validation Loss', linewidth=2, alpha=0.8)\n",
    "ax1.set_title('訓練與驗證損失曲線', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 繪製 Accuracy 曲線\n",
    "ax2.plot(history['train_acc'], label='Training Accuracy', linewidth=2, alpha=0.8)\n",
    "ax2.plot(history['valid_acc'], label='Validation Accuracy', linewidth=2, alpha=0.8)\n",
    "ax2.set_title('訓練與驗證準確率曲線', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 繪製學習率變化\n",
    "ax3.plot(history['lr'], linewidth=2, color='orange', alpha=0.8)\n",
    "ax3.set_title('學習率變化曲線', fontsize=16, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')  # 使用對數刻度\n",
    "\n",
    "# 繪製過擬合檢測圖 (訓練與驗證損失差異)\n",
    "loss_diff = np.array(history['train_loss']) - np.array(history['valid_loss'])\n",
    "ax4.plot(loss_diff, linewidth=2, color='red', alpha=0.8)\n",
    "ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.set_title('過擬合檢測 (訓練損失 - 驗證損失)', fontsize=16, fontweight='bold')\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Loss Difference', fontsize=12)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 輸出訓練統計資訊\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"訓練統計摘要:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"總訓練 epochs: {len(history['train_loss'])}\")\n",
    "print(f\"最佳訓練準確率: {max(history['train_acc']):.4f}\")\n",
    "print(f\"最佳驗證準確率: {max(history['valid_acc']):.4f}\")\n",
    "print(f\"最終訓練損失: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"最終驗證損失: {history['valid_loss'][-1]:.4f}\")\n",
    "print(f\"最終學習率: {history['lr'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d073ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 驗證集分類報告 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       778\n",
      "           1       0.17      0.53      0.26        40\n",
      "\n",
      "    accuracy                           0.85       818\n",
      "   macro avg       0.57      0.70      0.59       818\n",
      "weighted avg       0.93      0.85      0.89       818\n",
      "\n",
      "\n",
      "--- 驗證集混淆矩陣 ---\n",
      "                     Predicted Negative (0)  Predicted Positive (1)\n",
      "Actual Negative (0)                     677                     101\n",
      "Actual Positive (1)                      19                      21\n"
     ]
    }
   ],
   "source": [
    "# --- 載入最佳模型並進行詳細評估 ---\n",
    "\n",
    "print(\"載入最佳模型進行評估...\")\n",
    "\n",
    "# 1. 載入儲存的最佳模型權重\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"已載入最佳模型 (Epoch {checkpoint['epoch']+1}, 準確率: {checkpoint['best_acc']:.4f})\")\n",
    "\n",
    "# 2. 進行預測\n",
    "y_val_preds = []\n",
    "y_val_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in testloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        y_val_preds.extend(predicted.cpu().numpy())\n",
    "        y_val_probs.extend(probabilities[:, 1].cpu().numpy())  # 正類的機率\n",
    "\n",
    "y_val_preds = np.array(y_val_preds)\n",
    "y_val_probs = np.array(y_val_probs)\n",
    "\n",
    "# 3. 計算詳細評估指標\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_val_preds)\n",
    "precision = precision_score(y_val, y_val_preds)\n",
    "recall = recall_score(y_val, y_val_preds)\n",
    "f1 = f1_score(y_val, y_val_preds)\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_val, y_val_probs)\n",
    "except:\n",
    "    auc_score = None\n",
    "\n",
    "# 4. 顯示詳細評估結果\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"詳細評估結果:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"準確率 (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"精確率 (Precision): {precision:.4f}\")\n",
    "print(f\"召回率 (Recall): {recall:.4f}\")\n",
    "print(f\"F1 分數: {f1:.4f}\")\n",
    "if auc_score:\n",
    "    print(f\"AUC-ROC 分數: {auc_score:.4f}\")\n",
    "\n",
    "# 5. 顯示分類報告\n",
    "print(\"\\n--- 詳細分類報告 ---\")\n",
    "report = classification_report(y_true=y_val, y_pred=y_val_preds, \n",
    "                             target_names=['無中風', '中風'])\n",
    "print(report)\n",
    "\n",
    "# 6. 視覺化混淆矩陣\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 混淆矩陣\n",
    "cnfm = confusion_matrix(y_true=y_val, y_pred=y_val_preds)\n",
    "sns.heatmap(cnfm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['預測：無中風', '預測：中風'],\n",
    "            yticklabels=['實際：無中風', '實際：中風'],\n",
    "            ax=ax1)\n",
    "ax1.set_title('混淆矩陣', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 預測機率分布\n",
    "ax2.hist(y_val_probs[y_val == 0], bins=30, alpha=0.7, label='無中風', density=True)\n",
    "ax2.hist(y_val_probs[y_val == 1], bins=30, alpha=0.7, label='中風', density=True)\n",
    "ax2.set_xlabel('預測中風機率', fontsize=12)\n",
    "ax2.set_ylabel('密度', fontsize=12)\n",
    "ax2.set_title('預測機率分布', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. 顯示混淆矩陣數據表\n",
    "print(\"\\n--- 混淆矩陣詳細數據 ---\")\n",
    "cnfm_df = pd.DataFrame(cnfm, \n",
    "                      columns=['預測：無中風 (0)', '預測：中風 (1)'], \n",
    "                      index=['實際：無中風 (0)', '實際：中風 (1)'])\n",
    "print(cnfm_df)\n",
    "\n",
    "# 8. 計算並顯示更多統計資訊\n",
    "tn, fp, fn, tp = cnfm.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\n--- 詳細統計指標 ---\")\n",
    "print(f\"真陰性 (TN): {tn}\")\n",
    "print(f\"偽陽性 (FP): {fp}\")\n",
    "print(f\"偽陰性 (FN): {fn}\")\n",
    "print(f\"真陽性 (TP): {tp}\")\n",
    "print(f\"特異性 (Specificity): {specificity:.4f}\")\n",
    "print(f\"敏感性 (Sensitivity/Recall): {sensitivity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be454fc",
   "metadata": {},
   "source": [
    "## 7. 產生預測提交檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b392a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測結果已成功儲存至: ./submission.csv\n",
      "--- 提交檔案預覽 ---\n",
      "      id  stroke\n",
      "0  28326       1\n",
      "1   4607       0\n",
      "2  21206       0\n",
      "3  18020       0\n",
      "4  38354       0\n"
     ]
    }
   ],
   "source": [
    "# --- 對測試集進行預測並產生提交檔案 ---\n",
    "\n",
    "print(\"開始對測試集進行預測...\")\n",
    "\n",
    "try:\n",
    "    # 1. 確保載入最佳模型\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. 準備測試資料 (使用標準化後的資料)\n",
    "    X_test_tensor = torch.from_numpy(X_test_scaled.values.astype(np.float32)).to(device)\n",
    "    \n",
    "    # 3. 進行預測\n",
    "    test_predictions = []\n",
    "    test_probabilities = []\n",
    "    \n",
    "    # 批次處理以節省記憶體\n",
    "    batch_size = 1000\n",
    "    num_samples = len(X_test_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch_data = X_test_tensor[i:batch_end]\n",
    "            \n",
    "            outputs = model(batch_data)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "            test_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "    \n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_probabilities = np.array(test_probabilities)\n",
    "    \n",
    "    # 4. 建立詳細的提交檔案\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': data_test['id'],\n",
    "        'stroke': test_predictions,\n",
    "        'stroke_probability': test_probabilities  # 額外添加預測機率\n",
    "    })\n",
    "    \n",
    "    # 5. 儲存主要提交檔案 (只包含 id 和 stroke)\n",
    "    main_submission = submission_df[['id', 'stroke']].copy()\n",
    "    submission_path = './submission.csv'\n",
    "    main_submission.to_csv(submission_path, index=False)\n",
    "    \n",
    "    # 6. 儲存詳細版本 (包含機率)\n",
    "    detailed_submission_path = './detailed_submission.csv'\n",
    "    submission_df.to_csv(detailed_submission_path, index=False)\n",
    "    \n",
    "    # 7. 預測結果分析\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"預測結果分析:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"總測試樣本數: {len(test_predictions)}\")\n",
    "    print(f\"預測為中風的樣本數: {np.sum(test_predictions == 1)}\")\n",
    "    print(f\"預測為無中風的樣本數: {np.sum(test_predictions == 0)}\")\n",
    "    print(f\"中風預測比例: {np.mean(test_predictions):.4f}\")\n",
    "    print(f\"平均中風機率: {np.mean(test_probabilities):.4f}\")\n",
    "    print(f\"中風機率標準差: {np.std(test_probabilities):.4f}\")\n",
    "    \n",
    "    # 8. 機率分布統計\n",
    "    print(f\"\\n機率分布統計:\")\n",
    "    print(f\"最小機率: {np.min(test_probabilities):.4f}\")\n",
    "    print(f\"第25百分位數: {np.percentile(test_probabilities, 25):.4f}\")\n",
    "    print(f\"中位數: {np.median(test_probabilities):.4f}\")\n",
    "    print(f\"第75百分位數: {np.percentile(test_probabilities, 75):.4f}\")\n",
    "    print(f\"最大機率: {np.max(test_probabilities):.4f}\")\n",
    "    \n",
    "    # 9. 視覺化預測結果\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 預測類別分布\n",
    "    plt.subplot(1, 3, 1)\n",
    "    pred_counts = pd.Series(test_predictions).value_counts().sort_index()\n",
    "    plt.bar(['無中風', '中風'], pred_counts.values, color=['skyblue', 'lightcoral'])\n",
    "    plt.title('測試集預測結果分布', fontweight='bold')\n",
    "    plt.ylabel('樣本數')\n",
    "    \n",
    "    # 預測機率分布\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(test_probabilities, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.title('預測機率分布', fontweight='bold')\n",
    "    plt.xlabel('中風機率')\n",
    "    plt.ylabel('頻率')\n",
    "    \n",
    "    # 高風險樣本分析\n",
    "    plt.subplot(1, 3, 3)\n",
    "    high_risk_threshold = 0.7\n",
    "    risk_categories = ['低風險\\n(<0.3)', '中風險\\n(0.3-0.7)', '高風險\\n(>0.7)']\n",
    "    risk_counts = [\n",
    "        np.sum(test_probabilities < 0.3),\n",
    "        np.sum((test_probabilities >= 0.3) & (test_probabilities <= high_risk_threshold)),\n",
    "        np.sum(test_probabilities > high_risk_threshold)\n",
    "    ]\n",
    "    plt.bar(risk_categories, risk_counts, color=['green', 'orange', 'red'], alpha=0.7)\n",
    "    plt.title('風險等級分布', fontweight='bold')\n",
    "    plt.ylabel('樣本數')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 10. 檔案儲存確認\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"檔案儲存結果:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ 主要提交檔案已儲存至: {submission_path}\")\n",
    "    print(f\"✓ 詳細提交檔案已儲存至: {detailed_submission_path}\")\n",
    "    \n",
    "    # 11. 顯示提交檔案預覽\n",
    "    print(\"\\n--- 主要提交檔案預覽 ---\")\n",
    "    print(main_submission.head(10))\n",
    "    \n",
    "    print(\"\\n--- 詳細提交檔案預覽 ---\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    print(f\"\\n預測完成！共產生 {len(test_predictions)} 筆預測結果。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"預測過程中發生錯誤: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
